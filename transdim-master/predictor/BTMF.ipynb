{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Temporal Matrix Factorization\n",
    "\n",
    "**Published**: October 8, 2019\n",
    "\n",
    "**Revised**: October 8, 2020\n",
    "\n",
    "**Author**: Xinyu Chen [[**GitHub homepage**](https://github.com/xinychen)]\n",
    "\n",
    "**Download**: This Jupyter notebook is at our GitHub repository. If you want to evaluate the code, please download the notebook from the [**transdim**](https://github.com/xinychen/transdim/blob/master/predictor/BTMF.ipynb) repository.\n",
    "\n",
    "This notebook shows how to implement the Bayesian Temporal Matrix Factorization (BTMF), a fully Bayesian matrix factorization model, on some real-world data sets. To overcome the missing data problem in multivariate time series, BTMF takes into account both low-rank matrix structure and time series autoregression. For an in-depth discussion of BTMF, please see [1].\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=\"black\">\n",
    "<b>[1]</b> Xinyu Chen, Lijun Sun (2019). <b>Bayesian temporal factorization for multidimensional time series prediction</b>. arXiv:1910.06366. <a href=\"https://arxiv.org/pdf/1910.06366.pdf\" title=\"PDF\"><b>[PDF]</b></a> \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "\n",
    "We assume a spatiotemporal setting for multidimensional time series data throughout this work. In general, modern spatiotemporal data sets collected from sensor networks can be organized as matrix time series. For example, we can denote by matrix $Y\\in\\mathbb{R}^{N\\times T}$ a multivariate time series collected from $N$ locations/sensors on $T$ time points, with each row $$\\boldsymbol{y}_{i}=\\left(y_{i,1},y_{i,2},...,y_{i,t-1},y_{i,t},y_{i,t+1},...,y_{i,T}\\right)$$\n",
    "corresponding to the time series collected at location $i$.\n",
    "\n",
    "As mentioned, making accurate predictions on incomplete time series is very challenging, while missing data problem is almost inevitable in real-world applications. Figure 1 illustrates the prediction problem for incomplete time series data. Here we use $(i,t)\\in\\Omega$ to index the observed entries in matrix $Y$.\n",
    "\n",
    "<img src=\"../images/graphical_matrix_time_series.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "> **Figure 1**: Illustration of multivariate time series and the prediction problem in the presence of missing values (green: observed data; white: missing data; red: prediction).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv as inv\n",
    "from numpy.random import normal as normrnd\n",
    "from numpy.random import multivariate_normal as mvnrnd\n",
    "from scipy.linalg import khatri_rao as kr_prod\n",
    "from scipy.stats import wishart\n",
    "from scipy.stats import invwishart\n",
    "from numpy.linalg import solve as solve\n",
    "from numpy.linalg import cholesky as cholesky_lower\n",
    "from scipy.linalg import cholesky as cholesky_upper\n",
    "from scipy.linalg import solve_triangular as solve_ut\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mvnrnd_pre(mu, Lambda):\n",
    "    src = normrnd(size = (mu.shape[0],))\n",
    "    return solve_ut(cholesky_upper(Lambda, overwrite_a = True, check_finite = False), \n",
    "                    src, lower = False, check_finite = False, overwrite_b = True) + mu\n",
    "\n",
    "def cov_mat(mat, mat_bar):\n",
    "    mat = mat - mat_bar\n",
    "    return mat.T @ mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_factor_w(tau_sparse_mat, tau_ind, W, X, tau, beta0 = 1, vargin = 0):\n",
    "    \"\"\"Sampling N-by-R factor matrix W and its hyperparameters (mu_w, Lambda_w).\"\"\"\n",
    "    \n",
    "    dim1, rank = W.shape\n",
    "    W_bar = np.mean(W, axis = 0)\n",
    "    temp = dim1 / (dim1 + beta0)\n",
    "    var_W_hyper = inv(np.eye(rank) + cov_mat(W, W_bar) + temp * beta0 * np.outer(W_bar, W_bar))\n",
    "    var_Lambda_hyper = wishart.rvs(df = dim1 + rank, scale = var_W_hyper)\n",
    "    var_mu_hyper = mvnrnd_pre(temp * W_bar, (dim1 + beta0) * var_Lambda_hyper)\n",
    "    \n",
    "    if dim1 * rank ** 2 > 1e+8:\n",
    "        vargin = 1\n",
    "    \n",
    "    if vargin == 0:\n",
    "        var1 = X.T\n",
    "        var2 = kr_prod(var1, var1)\n",
    "        var3 = (var2 @ tau_ind.T).reshape([rank, rank, dim1]) + var_Lambda_hyper[:, :, None]\n",
    "        var4 = var1 @ tau_sparse_mat.T + (var_Lambda_hyper @ var_mu_hyper)[:, None]\n",
    "        for i in range(dim1):\n",
    "            W[i, :] = mvnrnd_pre(solve(var3[:, :, i], var4[:, i]), var3[:, :, i])\n",
    "    elif vargin == 1:\n",
    "        for i in range(dim1):\n",
    "            pos0 = np.where(sparse_mat[i, :] != 0)\n",
    "            Xt = X[pos0[0], :]\n",
    "            var_mu = tau[i] * Xt.T @ sparse_mat[i, pos0[0]] + var_Lambda_hyper @ var_mu_hyper\n",
    "            var_Lambda = tau[i] * Xt.T @ Xt + var_Lambda_hyper\n",
    "            W[i, :] = mvnrnd_pre(solve(var_Lambda, var_mu), var_Lambda)\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnrnd(M, U, V):\n",
    "    \"\"\"\n",
    "    Generate matrix normal distributed random matrix.\n",
    "    M is a m-by-n matrix, U is a m-by-m matrix, and V is a n-by-n matrix.\n",
    "    \"\"\"\n",
    "    dim1, dim2 = M.shape\n",
    "    X0 = np.random.randn(dim1, dim2)\n",
    "    P = cholesky_lower(U)\n",
    "    Q = cholesky_lower(V)\n",
    "    \n",
    "    return M + P @ X0 @ Q.T\n",
    "\n",
    "def sample_var_coefficient(X, time_lags):\n",
    "    dim, rank = X.shape\n",
    "    d = time_lags.shape[0]\n",
    "    tmax = np.max(time_lags)\n",
    "    \n",
    "    Z_mat = X[tmax : dim, :]\n",
    "    Q_mat = np.zeros((dim - tmax, rank * d))\n",
    "    for k in range(d):\n",
    "        Q_mat[:, k * rank : (k + 1) * rank] = X[tmax - time_lags[k] : dim - time_lags[k], :]\n",
    "    var_Psi0 = np.eye(rank * d) + Q_mat.T @ Q_mat\n",
    "    var_Psi = inv(var_Psi0)\n",
    "    var_M = var_Psi @ Q_mat.T @ Z_mat\n",
    "    var_S = np.eye(rank) + Z_mat.T @ Z_mat - var_M.T @ var_Psi0 @ var_M\n",
    "    Sigma = invwishart.rvs(df = rank + dim - tmax, scale = var_S)\n",
    "    \n",
    "    return mnrnd(var_M, var_Psi, Sigma), Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_factor_x(tau_sparse_mat, tau_ind, time_lags, W, X, A, Lambda_x):\n",
    "    \"\"\"Sampling T-by-R factor matrix X.\"\"\"\n",
    "    \n",
    "    dim2, rank = X.shape\n",
    "    tmax = np.max(time_lags)\n",
    "    tmin = np.min(time_lags)\n",
    "    d = time_lags.shape[0]\n",
    "    A0 = np.dstack([A] * d)\n",
    "    for k in range(d):\n",
    "        A0[k * rank : (k + 1) * rank, :, k] = 0\n",
    "    mat0 = Lambda_x @ A.T\n",
    "    mat1 = np.einsum('kij, jt -> kit', A.reshape([d, rank, rank]), Lambda_x)\n",
    "    mat2 = np.einsum('kit, kjt -> ij', mat1, A.reshape([d, rank, rank]))\n",
    "    \n",
    "    var1 = W.T\n",
    "    var2 = kr_prod(var1, var1)\n",
    "    var3 = (var2 @ tau_ind).reshape([rank, rank, dim2]) + Lambda_x[:, :, None]\n",
    "    var4 = var1 @ tau_sparse_mat\n",
    "    for t in range(dim2):\n",
    "        Mt = np.zeros((rank, rank))\n",
    "        Nt = np.zeros(rank)\n",
    "        Qt = mat0 @ X[t - time_lags, :].reshape(rank * d)\n",
    "        index = list(range(0, d))\n",
    "        if t >= dim2 - tmax and t < dim2 - tmin:\n",
    "            index = list(np.where(t + time_lags < dim2))[0]\n",
    "        elif t < tmax:\n",
    "            Qt = np.zeros(rank)\n",
    "            index = list(np.where(t + time_lags >= tmax))[0]\n",
    "        if t < dim2 - tmin:\n",
    "            Mt = mat2.copy()\n",
    "            temp = np.zeros((rank * d, len(index)))\n",
    "            n = 0\n",
    "            for k in index:\n",
    "                temp[:, n] = X[t + time_lags[k] - time_lags, :].reshape(rank * d)\n",
    "                n += 1\n",
    "            temp0 = X[t + time_lags[index], :].T - np.einsum('ijk, ik -> jk', A0[:, :, index], temp)\n",
    "            Nt = np.einsum('kij, jk -> i', mat1[index, :, :], temp0)\n",
    "        \n",
    "        var3[:, :, t] = var3[:, :, t] + Mt\n",
    "        if t < tmax:\n",
    "            var3[:, :, t] = var3[:, :, t] - Lambda_x + np.eye(rank)\n",
    "        X[t, :] = mvnrnd_pre(solve(var3[:, :, t], var4[:, t] + Nt + Qt), var3[:, :, t])\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_precision_tau(sparse_mat, mat_hat, ind):\n",
    "    var_alpha = 1e-6 + 0.5 * np.sum(ind, axis = 1)\n",
    "    var_beta = 1e-6 + 0.5 * np.sum(((sparse_mat - mat_hat) ** 2) * ind, axis = 1)\n",
    "    return np.random.gamma(var_alpha, 1 / var_beta)\n",
    "\n",
    "def sample_precision_scalar_tau(sparse_mat, mat_hat, ind):\n",
    "    var_alpha = 1e-6 + 0.5 * np.sum(ind)\n",
    "    var_beta = 1e-6 + 0.5 * np.sum(((sparse_mat - mat_hat) ** 2) * ind)\n",
    "    return np.random.gamma(var_alpha, 1 / var_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def compute_mape(var, var_hat):\n",
    "    return np.sum(np.abs(var - var_hat) / var) / var.shape[0]\n",
    "\n",
    "def compute_rmse(var, var_hat):\n",
    "    return  np.sqrt(np.sum((var - var_hat) ** 2) / var.shape[0])\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "def compute_mape(var, var_hat):\n",
    "    valid_indices = ~np.isnan(var) & ~np.isnan(var_hat)\n",
    "    return np.sum(np.abs(var[valid_indices] - var_hat[valid_indices]) / var[valid_indices]) / valid_indices.sum()\n",
    "\n",
    "def compute_rmse(var, var_hat):\n",
    "    valid_indices = ~np.isnan(var) & ~np.isnan(var_hat)\n",
    "    return np.sqrt(np.sum((var[valid_indices] - var_hat[valid_indices]) ** 2) / valid_indices.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ar4cast(A, X, Sigma, time_lags, multi_step):\n",
    "    dim, rank = X.shape\n",
    "    d = time_lags.shape[0]\n",
    "    X_new = np.append(X, np.zeros((multi_step, rank)), axis = 0)\n",
    "    for t in range(multi_step):\n",
    "        var = A.T @ X_new[dim + t - time_lags, :].reshape(rank * d)\n",
    "        X_new[dim + t, :] = mvnrnd(var, Sigma)\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BTMF Implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter, multi_step = 1, option = \"factor\"):\n",
    "    \"\"\"Bayesian Temporal Matrix Factorization, BTMF.\"\"\"\n",
    "    \n",
    "    dim1, dim2 = sparse_mat.shape\n",
    "    d = time_lags.shape[0]\n",
    "    W = init[\"W\"]\n",
    "    X = init[\"X\"]\n",
    "    if np.isnan(sparse_mat).any() == False:\n",
    "        ind = sparse_mat != 0\n",
    "        pos_obs = np.where(ind)\n",
    "        pos_test = np.where((dense_mat != 0) & (sparse_mat == 0))\n",
    "    elif np.isnan(sparse_mat).any() == True:\n",
    "        pos_test = np.where((dense_mat != 0) & (np.isnan(sparse_mat)))\n",
    "        ind = ~np.isnan(sparse_mat)\n",
    "        pos_obs = np.where(ind)\n",
    "        sparse_mat[np.isnan(sparse_mat)] = 0\n",
    "    dense_test = dense_mat[pos_test]\n",
    "    del dense_mat\n",
    "    tau = np.ones(dim1)\n",
    "    W_plus = np.zeros((dim1, rank, gibbs_iter))\n",
    "    A_plus = np.zeros((rank * d, rank, gibbs_iter))\n",
    "    tau_plus = np.zeros((dim1, gibbs_iter))\n",
    "    Sigma_plus = np.zeros((rank, rank, gibbs_iter))\n",
    "    temp_hat = np.zeros(len(pos_test[0]))\n",
    "    show_iter = 500\n",
    "    mat_hat_plus = np.zeros((dim1, dim2))\n",
    "    X_plus = np.zeros((dim2 + multi_step, rank, gibbs_iter))\n",
    "    mat_new_plus = np.zeros((dim1, multi_step))\n",
    "    for it in range(burn_iter + gibbs_iter):\n",
    "        tau_ind = tau[:, None] * ind\n",
    "        tau_sparse_mat = tau[:, None] * sparse_mat\n",
    "        W = sample_factor_w(tau_sparse_mat, tau_ind, W, X, tau, beta0 = 1, vargin = 0)\n",
    "        A, Sigma = sample_var_coefficient(X, time_lags)\n",
    "        X = sample_factor_x(tau_sparse_mat, tau_ind, time_lags, W, X, A, inv(Sigma))\n",
    "        mat_hat = W @ X.T\n",
    "        if option == \"factor\":\n",
    "            tau = sample_precision_tau(sparse_mat, mat_hat, ind)\n",
    "        elif option == \"pca\":\n",
    "            tau = sample_precision_scalar_tau(sparse_mat, mat_hat, ind)\n",
    "            tau = tau * np.ones(dim1)\n",
    "        temp_hat += mat_hat[pos_test]\n",
    "        if (it + 1) % show_iter == 0 and it < burn_iter:\n",
    "            temp_hat = temp_hat / show_iter\n",
    "            print('Iter: {}'.format(it + 1))\n",
    "            print('MAPE: {:.6}'.format(compute_mape(dense_test, temp_hat)))\n",
    "            print('RMSE: {:.6}'.format(compute_rmse(dense_test, temp_hat)))\n",
    "            temp_hat = np.zeros(len(pos_test[0]))\n",
    "            print()\n",
    "        if it + 1 > burn_iter:\n",
    "            W_plus[:, :, it - burn_iter] = W\n",
    "            A_plus[:, :, it - burn_iter] = A\n",
    "            Sigma_plus[:, :, it - burn_iter] = Sigma\n",
    "            tau_plus[:, it - burn_iter] = tau\n",
    "            mat_hat_plus += mat_hat\n",
    "            X0 = ar4cast(A, X, Sigma, time_lags, multi_step)\n",
    "            X_plus[:, :, it - burn_iter] = X0\n",
    "            mat_new_plus += W @ X0[dim2 : dim2 + multi_step, :].T\n",
    "    mat_hat = mat_hat_plus / gibbs_iter\n",
    "    print('Imputation MAPE: {:.6}'.format(compute_mape(dense_test, mat_hat[:, : dim2][pos_test])))\n",
    "    print('Imputation RMSE: {:.6}'.format(compute_rmse(dense_test, mat_hat[:, : dim2][pos_test])))\n",
    "    print()\n",
    "    mat_hat = np.append(mat_hat, mat_new_plus / gibbs_iter, axis = 1)\n",
    "    mat_hat[mat_hat < 0] = 0\n",
    "    \n",
    "    return mat_hat, W_plus, X_plus, A_plus, Sigma_plus, tau_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_factor_x_partial(tau_sparse_mat, tau_ind, time_lags, W, X, A, Lambda_x, back_step):\n",
    "    \"\"\"Sampling T-by-R factor matrix X.\"\"\"\n",
    "    \n",
    "    dim2, rank = X.shape\n",
    "    tmax = np.max(time_lags)\n",
    "    tmin = np.min(time_lags)\n",
    "    d = time_lags.shape[0]\n",
    "    A0 = np.dstack([A] * d)\n",
    "    for k in range(d):\n",
    "        A0[k * rank : (k + 1) * rank, :, k] = 0\n",
    "    mat0 = Lambda_x @ A.T\n",
    "    mat1 = np.einsum('kij, jt -> kit', A.reshape([d, rank, rank]), Lambda_x)\n",
    "    mat2 = np.einsum('kit, kjt -> ij', mat1, A.reshape([d, rank, rank]))\n",
    "    \n",
    "    var1 = W.T\n",
    "    var2 = kr_prod(var1, var1)\n",
    "    var3 = (var2 @ tau_ind[:, - back_step :]).reshape([rank, rank, back_step]) + Lambda_x[:, :, None]\n",
    "    var4 = var1 @ tau_sparse_mat[:, - back_step :]\n",
    "    for t in range(dim2 - back_step, dim2):\n",
    "        Mt = np.zeros((rank, rank))\n",
    "        Nt = np.zeros(rank)\n",
    "        Qt = mat0 @ X[t - time_lags, :].reshape(rank * d)\n",
    "        index = list(range(0, d))\n",
    "        if t >= dim2 - tmax and t < dim2 - tmin:\n",
    "            index = list(np.where(t + time_lags < dim2))[0]\n",
    "        if t < dim2 - tmin:\n",
    "            Mt = mat2.copy()\n",
    "            temp = np.zeros((rank * d, len(index)))\n",
    "            n = 0\n",
    "            for k in index:\n",
    "                temp[:, n] = X[t + time_lags[k] - time_lags, :].reshape(rank * d)\n",
    "                n += 1\n",
    "            temp0 = X[t + time_lags[index], :].T - np.einsum('ijk, ik -> jk', A0[:, :, index], temp)\n",
    "            Nt = np.einsum('kij, jk -> i', mat1[index, :, :], temp0)\n",
    "        var3[:, :, t + back_step - dim2] = var3[:, :, t + back_step - dim2] + Mt\n",
    "        X[t, :] = mvnrnd_pre(solve(var3[:, :, t + back_step - dim2], \n",
    "                                   var4[:, t + back_step - dim2] + Nt + Qt), var3[:, :, t + back_step - dim2])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BTMF_partial(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter, multi_step = 1, gamma = 10):\n",
    "    \"\"\"Bayesian Temporal Matrix Factorization, BTMF.\"\"\"\n",
    "    \n",
    "    dim1, dim2 = sparse_mat.shape\n",
    "    W_plus = init[\"W_plus\"]\n",
    "    X_plus = init[\"X_plus\"]\n",
    "    A_plus = init[\"A_plus\"]\n",
    "    Sigma_plus = init[\"Sigma_plus\"]\n",
    "    tau_plus = init[\"tau_plus\"]\n",
    "    if np.isnan(sparse_mat).any() == False:\n",
    "        ind = sparse_mat != 0\n",
    "        pos_obs = np.where(ind)\n",
    "    elif np.isnan(sparse_mat).any() == True:\n",
    "        ind = ~np.isnan(sparse_mat)\n",
    "        pos_obs = np.where(ind)\n",
    "        sparse_mat[np.isnan(sparse_mat)] = 0\n",
    "    X_new_plus = np.zeros((dim2 + multi_step, rank, gibbs_iter))\n",
    "    mat_new_plus = np.zeros((dim1, multi_step))\n",
    "    back_step = gamma * multi_step\n",
    "    for it in range(gibbs_iter):\n",
    "        tau_ind = tau_plus[:, it][:, None] * ind\n",
    "        tau_sparse_mat = tau_plus[:, it][:, None] * sparse_mat\n",
    "        X = sample_factor_x_partial(tau_sparse_mat, tau_ind, time_lags, W_plus[:, :, it], \n",
    "                                    X_plus[:, :, it], A_plus[:, :, it], inv(Sigma_plus[:, :, it]), back_step)\n",
    "        X0 = ar4cast(A_plus[:, :, it], X, Sigma_plus[:, :, it], time_lags, multi_step)\n",
    "        X_new_plus[:, :, it] = X0\n",
    "        mat_new_plus += W_plus[:, :, it] @ X0[- multi_step :, :].T\n",
    "    mat_hat = mat_new_plus / gibbs_iter\n",
    "    mat_hat[mat_hat < 0] = 0\n",
    "    \n",
    "    return mat_hat, W_plus, X_new_plus, A_plus, Sigma_plus, tau_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "def BTMF_forecast(dense_mat, sparse_mat, pred_step, multi_step, \n",
    "                  rank, time_lags, burn_iter, gibbs_iter, option = \"factor\", gamma = 10):\n",
    "    dim1, T = dense_mat.shape\n",
    "    start_time = T - pred_step\n",
    "    max_count = int(np.ceil(pred_step / multi_step))\n",
    "    mat_hat = np.zeros((dim1, max_count * multi_step))\n",
    "    #f = IntProgress(min = 0, max = max_count) # instantiate the bar\n",
    "    #display(f) # display the bar\n",
    "    for t in range(max_count):\n",
    "        if t == 0:\n",
    "            init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(start_time, rank)}\n",
    "            mat, W, X_new, A, Sigma, tau = BTMF(dense_mat[:, 0 : start_time], \n",
    "                sparse_mat[:, 0 : start_time], init, rank, time_lags, burn_iter, gibbs_iter, multi_step, option)\n",
    "        else:\n",
    "            init = {\"W_plus\": W, \"X_plus\": X_new, \"A_plus\": A, \"Sigma_plus\": Sigma, \"tau_plus\": tau}\n",
    "            mat, W, X_new, A, Sigma, tau = BTMF_partial(dense_mat[:, 0 : start_time + t * multi_step], \n",
    "                sparse_mat[:, 0 : start_time + t * multi_step], init, rank, time_lags, \n",
    "                burn_iter, gibbs_iter, multi_step, gamma)\n",
    "        mat_hat[:, t * multi_step : (t + 1) * multi_step] = mat[:, - multi_step :]\n",
    "        #f.value = t\n",
    "    small_dense_mat = dense_mat[:, start_time : T]\n",
    "    pos = np.where(small_dense_mat != 0)\n",
    "    print('Prediction MAPE: {:.6}'.format(compute_mape(small_dense_mat[pos], mat_hat[pos])))\n",
    "    print('Prediction RMSE: {:.6}'.format(compute_rmse(small_dense_mat[pos], mat_hat[pos])))\n",
    "    print()\n",
    "    return mat_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from distutils.util import strtobool\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Converts the contents in a .tsf file into a dataframe and returns it along with other meta-data of the dataset: frequency, horizon, whether the dataset contains missing values and whether the series have equal lengths\n",
    "#\n",
    "# Parameters\n",
    "# full_file_path_and_name - complete .tsf file path\n",
    "# replace_missing_vals_with - a term to indicate the missing values in series in the returning dataframe\n",
    "# value_column_name - Any name that is preferred to have as the name of the column containing series values in the returning dataframe\n",
    "def convert_tsf_to_dataframe(\n",
    "    full_file_path_and_name,\n",
    "    replace_missing_vals_with=\"NaN\",\n",
    "    value_column_name=\"series_value\",\n",
    "):\n",
    "    col_names = []\n",
    "    col_types = []\n",
    "    all_data = {}\n",
    "    line_count = 0\n",
    "    frequency = None\n",
    "    forecast_horizon = None\n",
    "    contain_missing_values = None\n",
    "    contain_equal_length = None\n",
    "    found_data_tag = False\n",
    "    found_data_section = False\n",
    "    started_reading_data_section = False\n",
    "\n",
    "    with open(full_file_path_and_name, \"r\", encoding=\"cp1252\") as file:\n",
    "        for line in file:\n",
    "            # Strip white space from start/end of line\n",
    "            line = line.strip()\n",
    "\n",
    "            if line:\n",
    "                if line.startswith(\"@\"):  # Read meta-data\n",
    "                    if not line.startswith(\"@data\"):\n",
    "                        line_content = line.split(\" \")\n",
    "                        if line.startswith(\"@attribute\"):\n",
    "                            if (\n",
    "                                len(line_content) != 3\n",
    "                            ):  # Attributes have both name and type\n",
    "                                raise Exception(\"Invalid meta-data specification.\")\n",
    "\n",
    "                            col_names.append(line_content[1])\n",
    "                            col_types.append(line_content[2])\n",
    "                        else:\n",
    "                            if (\n",
    "                                len(line_content) != 2\n",
    "                            ):  # Other meta-data have only values\n",
    "                                raise Exception(\"Invalid meta-data specification.\")\n",
    "\n",
    "                            if line.startswith(\"@frequency\"):\n",
    "                                frequency = line_content[1]\n",
    "                            elif line.startswith(\"@horizon\"):\n",
    "                                forecast_horizon = int(line_content[1])\n",
    "                            elif line.startswith(\"@missing\"):\n",
    "                                contain_missing_values = bool(\n",
    "                                    strtobool(line_content[1])\n",
    "                                )\n",
    "                            elif line.startswith(\"@equallength\"):\n",
    "                                contain_equal_length = bool(strtobool(line_content[1]))\n",
    "\n",
    "                    else:\n",
    "                        if len(col_names) == 0:\n",
    "                            raise Exception(\n",
    "                                \"Missing attribute section. Attribute section must come before data.\"\n",
    "                            )\n",
    "\n",
    "                        found_data_tag = True\n",
    "                elif not line.startswith(\"#\"):\n",
    "                    if len(col_names) == 0:\n",
    "                        raise Exception(\n",
    "                            \"Missing attribute section. Attribute section must come before data.\"\n",
    "                        )\n",
    "                    elif not found_data_tag:\n",
    "                        raise Exception(\"Missing @data tag.\")\n",
    "                    else:\n",
    "                        if not started_reading_data_section:\n",
    "                            started_reading_data_section = True\n",
    "                            found_data_section = True\n",
    "                            all_series = []\n",
    "\n",
    "                            for col in col_names:\n",
    "                                all_data[col] = []\n",
    "\n",
    "                        full_info = line.split(\":\")\n",
    "\n",
    "                        if len(full_info) != (len(col_names) + 1):\n",
    "                            raise Exception(\"Missing attributes/values in series.\")\n",
    "\n",
    "                        series = full_info[len(full_info) - 1]\n",
    "                        series = series.split(\",\")\n",
    "\n",
    "                        if len(series) == 0:\n",
    "                            raise Exception(\n",
    "                                \"A given series should contains a set of comma separated numeric values. At least one numeric value should be there in a series. Missing values should be indicated with ? symbol\"\n",
    "                            )\n",
    "\n",
    "                        numeric_series = []\n",
    "\n",
    "                        for val in series:\n",
    "                            if val == \"?\":\n",
    "                                numeric_series.append(replace_missing_vals_with)\n",
    "                            else:\n",
    "                                numeric_series.append(float(val))\n",
    "\n",
    "                        if numeric_series.count(replace_missing_vals_with) == len(\n",
    "                            numeric_series\n",
    "                        ):\n",
    "                            raise Exception(\n",
    "                                \"All series values are missing. A given series should contains a set of comma separated numeric values. At least one numeric value should be there in a series.\"\n",
    "                            )\n",
    "\n",
    "                        all_series.append(pd.Series(numeric_series).array)\n",
    "\n",
    "                        for i in range(len(col_names)):\n",
    "                            att_val = None\n",
    "                            if col_types[i] == \"numeric\":\n",
    "                                att_val = int(full_info[i])\n",
    "                            elif col_types[i] == \"string\":\n",
    "                                att_val = str(full_info[i])\n",
    "                            elif col_types[i] == \"date\":\n",
    "                                att_val = datetime.strptime(\n",
    "                                    full_info[i], \"%Y-%m-%d %H-%M-%S\"\n",
    "                                )\n",
    "                            else:\n",
    "                                raise Exception(\n",
    "                                    \"Invalid attribute type.\"\n",
    "                                )  # Currently, the code supports only numeric, string and date types. Extend this as required.\n",
    "\n",
    "                            if att_val is None:\n",
    "                                raise Exception(\"Invalid attribute value.\")\n",
    "                            else:\n",
    "                                all_data[col_names[i]].append(att_val)\n",
    "\n",
    "                line_count = line_count + 1\n",
    "\n",
    "        if line_count == 0:\n",
    "            raise Exception(\"Empty file.\")\n",
    "        if len(col_names) == 0:\n",
    "            raise Exception(\"Missing attribute section.\")\n",
    "        if not found_data_section:\n",
    "            raise Exception(\"Missing series information under data section.\")\n",
    "\n",
    "        all_data[value_column_name] = all_series\n",
    "        loaded_data = pd.DataFrame(all_data)\n",
    "\n",
    "        return (\n",
    "            loaded_data,\n",
    "            frequency,\n",
    "            forecast_horizon,\n",
    "            contain_missing_values,\n",
    "            contain_equal_length,\n",
    "        )\n",
    "\n",
    "\n",
    "# Example of usage\n",
    "# loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(\"TSForecasting/tsf_data/sample.tsf\")\n",
    "\n",
    "# print(loaded_data)\n",
    "# print(frequency)\n",
    "# print(forecast_horizon)\n",
    "# print(contain_missing_values)\n",
    "# print(contain_equal_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   series_name     start_timestamp  \\\n",
      "0    Building0 2016-07-03 21:30:00   \n",
      "1    Building1 2019-01-09 23:15:00   \n",
      "2    Building3 2016-03-01 04:15:00   \n",
      "3    Building4 2019-07-03 04:45:00   \n",
      "4    Building5 2019-07-25 23:00:00   \n",
      "5    Building6 2019-07-25 01:45:00   \n",
      "6       Solar0 2020-04-25 14:00:00   \n",
      "7       Solar1 2018-12-31 13:00:00   \n",
      "8       Solar2 2019-06-05 14:00:00   \n",
      "9       Solar3 2019-06-05 14:00:00   \n",
      "10      Solar4 2019-06-05 14:00:00   \n",
      "11      Solar5 2019-01-15 13:00:00   \n",
      "\n",
      "                                         series_value  \n",
      "0   [283.8, 283.8, 283.8, 606.0, 606.0, 606.0, 606...  \n",
      "1   [8.1, 15.7, 22.8, 32.7, 8.1, 16.5, 24.7, 34.5,...  \n",
      "2   [1321.0, 1321.0, 1321.0, 1321.0, 1293.0, 1293....  \n",
      "3   [2.0, NaN, 1.0, 2.0, NaN, 2.0, NaN, NaN, 2.0, ...  \n",
      "4   [30.0, 31.0, 24.0, 34.0, 30.0, 31.0, 26.0, 33....  \n",
      "5   [36.8, 34.6, 34.6, 36.2, 36.2, 35.2, 35.2, 35....  \n",
      "6   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "7   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "8   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "9   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "10  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "11  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "max timestamp 2019-07-25 23:00:00\n",
      "0    148810\n",
      "1     60483\n",
      "2    160783\n",
      "3     43757\n",
      "4     41572\n",
      "5     41657\n",
      "Name: num_timestamps, dtype: int64\n",
      "0    [NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, ...\n",
      "1    [26.6, 6.2, 12.7, 19.8, 26.0, 7.0, 14.3, 20.7,...\n",
      "2    [690.0, 795.0, 795.0, 795.0, 795.0, 867.0, 867...\n",
      "3    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, NaN, ...\n",
      "4    [30.0, 31.0, 24.0, 34.0, 30.0, 31.0, 26.0, 33....\n",
      "5    [40.0, 40.0, 35.6, 35.6, 37.6, 37.6, 37.4, 37....\n",
      "Name: uniform_series, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# 3. Calculate the number of 15-minute intervals to trim for each series\\nbuilding_data['intervals_to_trim'] = building_data['start_timestamp'].apply(lambda x: int((x - max_start_timestamp).total_seconds() // (15*60)))\\n\\n# 4. Trim the time series of each building from the beginning up to this timestamp\\nbuilding_data['trimmed_series'] = building_data.apply(lambda row: row['series_value'][row['intervals_to_trim']:], axis=1)\\n\\n# 5. Determine the minimum length of the trimmed series\\nmin_length = building_data['trimmed_series'].apply(len).min()\\n\\n# 6. Trim (or pad) all series to this minimum length\\nbuilding_data['uniform_series'] = building_data['trimmed_series'].apply(lambda x: x[:min_length])\\n\\n#print(building_data)\\n#print(building_data['uniform_series'].shape)\\n\\n# 7. Convert the uniform series to a matrix format\\nmatrix = np.array(building_data['uniform_series'].tolist())\\n\""
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the .tsf file\n",
    "\n",
    "# Convert to DataFrame\n",
    "\n",
    "loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(\"C:/Users/Rohit/Documents/Exeter-Placement/Challenge/phase_1 data/phase_1_data/phase_1_data.tsf\")\n",
    "print(loaded_data)\n",
    "#print(loaded_data.shape,frequency, forecast_horizon, contain_missing_values, contain_equal_length)\n",
    "building_data = loaded_data[loaded_data['series_name'].str.contains('Building')]\n",
    "#print(building_data)\n",
    "# 2. Determine the maximum starting timestamp among the buildings\n",
    "\n",
    "max_start_timestamp = building_data['start_timestamp'].max()\n",
    "print(\"max timestamp\",max_start_timestamp)\n",
    "building_data['num_timestamps'] = building_data['series_value'].apply(len)\n",
    "print(building_data['num_timestamps'])\n",
    "min_timestamps = building_data['num_timestamps'].min()\n",
    "\n",
    "# Trim each time series to have the same length as the shortest one\n",
    "building_data['uniform_series'] = building_data['series_value'].apply(lambda x: x[-min_timestamps:])\n",
    "\n",
    "# Update the 'num_timestamps' column to reflect the new uniform length\n",
    "building_data['num_timestamps'] = building_data['uniform_series'].apply(len)\n",
    "print(building_data['uniform_series'])\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# 3. Calculate the number of 15-minute intervals to trim for each series\n",
    "building_data['intervals_to_trim'] = building_data['start_timestamp'].apply(lambda x: int((x - max_start_timestamp).total_seconds() // (15*60)))\n",
    "\n",
    "# 4. Trim the time series of each building from the beginning up to this timestamp\n",
    "building_data['trimmed_series'] = building_data.apply(lambda row: row['series_value'][row['intervals_to_trim']:], axis=1)\n",
    "\n",
    "# 5. Determine the minimum length of the trimmed series\n",
    "min_length = building_data['trimmed_series'].apply(len).min()\n",
    "\n",
    "# 6. Trim (or pad) all series to this minimum length\n",
    "building_data['uniform_series'] = building_data['trimmed_series'].apply(lambda x: x[:min_length])\n",
    "\n",
    "#print(building_data)\n",
    "#print(building_data['uniform_series'].shape)\n",
    "\n",
    "# 7. Convert the uniform series to a matrix format\n",
    "matrix = np.array(building_data['uniform_series'].tolist())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense mat shape (6, 41572)\n",
      "[['NaN' 'NaN' 'NaN' ... 37.4 37.4 37.4]\n",
      " [26.6 6.2 12.7 ... 4.5 9.6 14.0]\n",
      " [690.0 795.0 795.0 ... 415.0 415.0 415.0]\n",
      " [1.0 1.0 1.0 ... 'NaN' 'NaN' 'NaN']\n",
      " [30.0 31.0 24.0 ... 'NaN' 'NaN' 'NaN']\n",
      " [40.0 40.0 35.6 ... 38.6 38.6 38.6]]\n",
      "Imputation MAPE: nan\n",
      "Imputation RMSE: nan\n",
      "\n",
      "matrix shape:  (6, 41573)\n",
      "[[9.89388274e+01 1.19279060e+02 9.64472698e+01 ... 5.64262620e+01\n",
      "  4.28970631e+01 8.21183590e+01]\n",
      " [1.20716302e+01 5.07504779e-01 1.33201054e-01 ... 9.55059753e+00\n",
      "  1.06349067e+01 1.07226635e+01]\n",
      " [6.67093732e+02 7.72243738e+02 7.73202511e+02 ... 4.12816264e+02\n",
      "  4.09021390e+02 3.85899473e+02]\n",
      " [8.93586070e-01 9.71074323e-01 6.18161199e-01 ... 1.17311426e+00\n",
      "  1.36551994e+00 1.51073500e+00]\n",
      " [1.64865657e+01 2.64837653e+01 2.05533586e+01 ... 1.88022812e+01\n",
      "  1.99855959e+01 1.89754550e+01]\n",
      " [2.64778321e+01 3.04297892e+01 2.15512089e+01 ... 2.84413574e+01\n",
      "  3.38253658e+01 3.26071116e+01]]\n",
      "starting prediction\n",
      "Prediction time horizon (delta) = 2.\n",
      "Imputation MAPE: nan\n",
      "Imputation RMSE: nan\n",
      "\n",
      "Prediction MAPE: 0.577802\n",
      "Prediction RMSE: 33.9489\n",
      "\n",
      "[[150.6890831  193.44607873 198.03242431 ... 156.6186548    0.\n",
      "    5.12634429]\n",
      " [  8.66468806   9.52789905   7.94042667 ...  11.22124994   9.17725411\n",
      "    9.46657274]\n",
      " [354.72325961 331.92631272 294.93295423 ... 401.91876159 409.80436987\n",
      "  457.17543475]\n",
      " [  1.05027769   1.37364103   1.27952971 ...   1.64989622   1.20753407\n",
      "    0.86832228]\n",
      " [ 14.92661293  16.51462421  14.26364305 ...  20.98642981  21.86346113\n",
      "   17.20457655]\n",
      " [ 26.23132661  31.04224208  26.48441078 ...  39.3229414   31.18409208\n",
      "   26.88333042]]\n",
      "Running time: 11 seconds\n",
      "\n",
      "Prediction time horizon (delta) = 4.\n",
      "Imputation MAPE: nan\n",
      "Imputation RMSE: nan\n",
      "\n",
      "Prediction MAPE: 0.708802\n",
      "Prediction RMSE: 46.9652\n",
      "\n",
      "[[217.29046461 187.83452543 181.26805608 ... 270.20799393 240.32685098\n",
      "  192.62271441]\n",
      " [  9.20020147   9.0776395    8.4579178  ...  10.09750256   9.39148816\n",
      "    9.06910643]\n",
      " [341.04694323 372.40693329 355.06894323 ... 451.84953019 388.4517189\n",
      "  378.79728237]\n",
      " [  1.2870254    1.4217571    1.29066382 ...   1.17916801   1.30021616\n",
      "    1.16305715]\n",
      " [ 17.26296538  22.70019208  20.47637758 ...  19.99266783  21.09280671\n",
      "   19.74908881]\n",
      " [ 28.68509032  31.48854536  28.06579527 ...  26.7439019   29.28215753\n",
      "   26.99113857]]\n",
      "Running time: 11 seconds\n",
      "\n",
      "Prediction time horizon (delta) = 6.\n",
      "Imputation MAPE: nan\n",
      "Imputation RMSE: nan\n",
      "\n",
      "Prediction MAPE: 1.09376\n",
      "Prediction RMSE: 46.935\n",
      "\n",
      "[[146.00178479 182.07577403 239.52737892 ...  13.75503162  39.83159575\n",
      "   12.90226414]\n",
      " [  7.52312335   5.75409897   8.56091673 ...   8.81483248   8.66082994\n",
      "    3.79900847]\n",
      " [346.51605558 333.5973156  363.54054349 ... 340.69994371 302.68736988\n",
      "  275.01488748]\n",
      " [  0.9860632    1.14111977   1.36162042 ...   1.06122442   1.23266056\n",
      "    1.05105362]\n",
      " [ 17.71518439  24.18534753  19.89309394 ...  15.49395229  16.40173041\n",
      "   17.02970296]\n",
      " [ 22.54362897  24.43460698  29.30024254 ...  27.67333015  31.59848943\n",
      "   23.50395917]]\n",
      "Running time: 11 seconds\n",
      "\n",
      "Prediction time horizon (delta) = 12.\n",
      "Imputation MAPE: nan\n",
      "Imputation RMSE: nan\n",
      "\n",
      "Prediction MAPE: 1.22892\n",
      "Prediction RMSE: 57.0837\n",
      "\n",
      "[[134.18084226 151.10224055 125.39950636 ... 115.86013269 162.90245269\n",
      "  145.34474239]\n",
      " [  6.94741119   7.72365809   6.8027721  ...  12.96166918  12.03594725\n",
      "   12.24726354]\n",
      " [335.60143029 318.18321393 328.41993431 ... 606.22950711 585.68330917\n",
      "  598.6857838 ]\n",
      " [  1.19413515   1.13083905   1.12306912 ...   1.50105059   1.30104566\n",
      "    1.06110493]\n",
      " [ 22.37324448  17.9657669   20.87154832 ...  26.61846583  24.58560838\n",
      "   24.84065715]\n",
      " [ 28.46849396  24.93087113  24.25424189 ...  36.21440814  33.1592906\n",
      "   27.74064565]]\n",
      "Running time: 11 seconds\n",
      "\n",
      "Prediction time horizon (delta) = 18.\n",
      "Imputation MAPE: nan\n",
      "Imputation RMSE: nan\n",
      "\n",
      "Prediction MAPE: 0.822112\n",
      "Prediction RMSE: 70.5032\n",
      "\n",
      "[[150.09044559 128.65009738 119.12256583 ...  61.27205687 107.65774392\n",
      "  101.97977526]\n",
      " [  7.36584636   9.39987385   7.63131697 ...   7.65302958   6.53870599\n",
      "    7.79493685]\n",
      " [316.44004631 330.17278116 289.23241225 ... 338.55065129 331.69088141\n",
      "  302.27673427]\n",
      " [  1.31224518   1.13705923   1.37339156 ...   0.66348279   0.72760299\n",
      "    0.73028805]\n",
      " [ 22.30466327  13.50068077  19.33830101 ...   9.20111887  12.35988075\n",
      "    9.41949101]\n",
      " [ 31.24922981  27.23534994  33.7341814  ...  15.81815009  17.27548537\n",
      "   15.18723722]]\n",
      "Running time: 10 seconds\n",
      "\n",
      "Prediction time horizon (delta) = 24.\n",
      "Imputation MAPE: nan\n",
      "Imputation RMSE: nan\n",
      "\n",
      "Prediction MAPE: 0.581949\n",
      "Prediction RMSE: 55.6064\n",
      "\n",
      "[[166.40650233 219.40474907 270.55221834 ... 245.83154831 223.11496312\n",
      "  213.64750555]\n",
      " [  8.47747257   8.30591586   7.91531042 ...   8.92457321   7.04986212\n",
      "    6.9189916 ]\n",
      " [335.85648901 303.83267391 353.58696984 ... 337.39466296 301.97655291\n",
      "  276.81211524]\n",
      " [  1.28281109   1.31215695   1.31309588 ...   1.31983581   1.00406214\n",
      "    1.12588273]\n",
      " [ 19.07562671   9.42671255  21.67276968 ...  15.18817418  18.09440016\n",
      "   19.26474727]\n",
      " [ 28.09198729  27.9355482   28.14181459 ...  29.3183963   23.15373221\n",
      "   24.96446307]]\n",
      "Running time: 10 seconds\n",
      "\n",
      "(6, 264)\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Assuming loaded_data is already prepared and is in the format of a dense matrix\n",
    "dense_mat = building_data['uniform_series']  # Adjust this based on how you've prepared loaded_data\n",
    "\n",
    "  # Assuming you want to work on a copy of the original data\n",
    "\n",
    "list_of_arrays = [np.array(series) for series in dense_mat]\n",
    "\n",
    "# Stack these arrays vertically to form a 2D matrix\n",
    "dense_mat_2d = np.vstack(list_of_arrays)\n",
    "sparse_mat = dense_mat_2d.copy()\n",
    "print(\"dense mat shape\",dense_mat_2d.shape)\n",
    "print(dense_mat_2d)\n",
    "dense_mat_2d = np.where(dense_mat_2d == 'NaN', np.nan, dense_mat_2d).astype(float)\n",
    "sparse_mat = np.where(sparse_mat == 'NaN', np.nan, sparse_mat).astype(float)\n",
    "\n",
    "# Model Setting\n",
    "rank = 10\n",
    "pred_step = 248  # You mentioned you want to predict the next 248 energy demand values\n",
    "time_lags = np.array([1, 2, 48])  # Adjust this based on the seasonality or patterns in your data#\n",
    "burn_iter = 2\n",
    "gibbs_iter = 1\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
    "BTMFburn=2\n",
    "BTMFgibbs=1\n",
    "matrix, W_plus, X_plus, A_plus, Sigma_plus, tau_plus=BTMF(dense_mat_2d, sparse_mat, init, rank, time_lags, BTMFburn, BTMFgibbs, multi_step = 1, option = \"factor\")\n",
    "print(\"matrix shape: \", matrix.shape)\n",
    "print(matrix)\n",
    "df = pd.DataFrame(matrix)  # Assuming mat_hat is your data matrix\n",
    "df.to_csv('C:/Users/Rohit/Documents/Exeter-Placement/Results/building_predicted_data.csv', index=False)\n",
    "print(\"starting prediction\")\n",
    "# Apply BTMF forecasting for different prediction time horizons (if needed)\n",
    "for multi_step in [2, 4, 6, 12, 18, 24]:  # Adjust this list based on your requirements\n",
    "    start = time.time()\n",
    "    print('Prediction time horizon (delta) = {}.'.format(multi_step))\n",
    "    building_mat_hat = BTMF_forecast(dense_mat_2d, sparse_mat, pred_step, multi_step, rank, time_lags, burn_iter, gibbs_iter)\n",
    "    print(building_mat_hat)\n",
    "    end = time.time()\n",
    "    print('Running time: %d seconds'%(end - start))\n",
    "    print()\n",
    "print(building_mat_hat.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max timestamp 2020-04-25 14:00:00\n",
      "6     15208\n",
      "7     61388\n",
      "8     46408\n",
      "9     46408\n",
      "10    46408\n",
      "11    59948\n",
      "Name: num_timestamps, dtype: int64\n",
      "6     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "7     [7.63, 7.63, 7.63, 7.63, 7.63, 7.63, 7.63, 7.6...\n",
      "8     [3.75, 3.75, 3.75, 3.75, 3.75, 3.75, 3.75, 3.7...\n",
      "9     [1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.3...\n",
      "10    [1.66, 1.66, 1.66, 1.66, 1.66, 1.66, 1.66, 1.6...\n",
      "11    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "Name: uniform_series, dtype: object\n",
      "6     66708.91\n",
      "7     39352.41\n",
      "8     28946.03\n",
      "9     20135.41\n",
      "10    14760.45\n",
      "11    68998.96\n",
      "Name: uniform_series, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the .tsf file\n",
    "\n",
    "# Convert to DataFrame\n",
    "loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(\"C:/Users/Rohit/Documents/Exeter-Placement/Challenge/phase_1 data/phase_1_data/phase_1_data.tsf\")\n",
    "#print(loaded_data.shape,frequency, forecast_horizon, contain_missing_values, contain_equal_length)\n",
    "solar_data = loaded_data[loaded_data['series_name'].str.contains('Solar')]\n",
    "#print(building_data)\n",
    "# 2. Determine the maximum starting timestamp among the buildings\n",
    "\n",
    "max_start_timestamp = solar_data['start_timestamp'].max()\n",
    "print(\"max timestamp\",max_start_timestamp)\n",
    "solar_data['num_timestamps'] = solar_data['series_value'].apply(len)\n",
    "print(solar_data['num_timestamps'])\n",
    "min_timestamps = solar_data['num_timestamps'].min()\n",
    "\n",
    "# Trim each time series to have the same length as the shortest one\n",
    "solar_data['uniform_series'] = solar_data['series_value'].apply(lambda x: x[-min_timestamps:])\n",
    "\n",
    "# Update the 'num_timestamps' column to reflect the new uniform length\n",
    "solar_data['num_timestamps'] = solar_data['uniform_series'].apply(len)\n",
    "print(solar_data['uniform_series'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense mat shape (6, 15208)\n",
      "[[ 0.    0.    0.   ... 34.02 38.1  39.88]\n",
      " [ 7.63  7.63  7.63 ...  8.13  9.55 10.12]\n",
      " [ 3.75  3.75  3.75 ...  7.71  9.03  9.67]\n",
      " [ 1.34  1.34  1.34 ...  6.83  8.02  8.62]\n",
      " [ 1.66  1.66  1.66 ...  5.32  6.08  6.33]\n",
      " [ 0.    0.    0.   ... 28.62 31.94 32.92]]\n",
      "Imputation MAPE: nan\n",
      "Imputation RMSE: nan\n",
      "\n",
      "matrix shape:  (6, 15209)\n",
      "[[ 4.10297556  4.11755539  3.74446215 ... 38.19053402 39.88572917\n",
      "  43.28053919]\n",
      " [ 7.20926065  7.17141986  7.16476544 ...  9.6737489  10.19980158\n",
      "  10.79565532]\n",
      " [ 3.77907086  3.76317328  3.74166275 ...  9.13132746  9.64370193\n",
      "  10.32781094]\n",
      " [ 1.41392267  1.50512768  1.38865753 ...  8.05979716  8.63357836\n",
      "   9.42415255]\n",
      " [ 1.69419594  1.69057853  1.705492   ...  6.04250152  6.30859442\n",
      "   6.64165907]\n",
      " [16.26808539 13.47174915 14.69966345 ... 33.18396789 35.16497211\n",
      "  37.52875159]]\n",
      "starting prediction\n",
      "Prediction time horizon (delta) = 4.\n",
      "Imputation MAPE: nan\n",
      "Imputation RMSE: nan\n",
      "\n",
      "Prediction MAPE: 5.87552\n",
      "Prediction RMSE: 2.42796\n",
      "\n",
      "[[33.55256223 35.22119405 35.94779026 ... 34.53700162 37.24906509\n",
      "  38.71849787]\n",
      " [ 8.83334989  9.24651989 10.07335707 ...  8.83973445  9.66347968\n",
      "  10.06823504]\n",
      " [ 8.07762979  8.63236193  9.16526322 ...  8.19271667  9.07332212\n",
      "   9.41754521]\n",
      " [ 7.32160506  7.8375068   8.06919932 ...  7.4218266   8.24899215\n",
      "   8.64736491]\n",
      " [ 5.14735658  5.34739591  5.59560939 ...  5.23341624  5.5580195\n",
      "   5.88517443]\n",
      " [31.07519617 32.26487635 33.23916109 ... 29.40645346 32.84651061\n",
      "  34.30540893]]\n",
      "Running time: 44 seconds\n",
      "\n",
      "Prediction time horizon (delta) = 24.\n",
      "Imputation MAPE: nan\n",
      "Imputation RMSE: nan\n",
      "\n",
      "Prediction MAPE: 8.25245\n",
      "Prediction RMSE: 5.62237\n",
      "\n",
      "[[32.49101001 34.66906297 37.1402761  ... 10.63516244 12.95136721\n",
      "  14.87602034]\n",
      " [ 8.62525976  9.2120196   9.92291462 ...  3.43535586  3.62784882\n",
      "   4.02296234]\n",
      " [ 7.84406619  8.36492885  9.21217081 ...  3.06876365  3.53948782\n",
      "   3.91057083]\n",
      " [ 7.12084662  7.71813274  8.49671096 ...  2.75519137  3.31423722\n",
      "   3.61299197]\n",
      " [ 5.14515122  5.42441181  5.74638089 ...  1.88345825  2.1120623\n",
      "   2.36683341]\n",
      " [31.23475147 32.26701302 34.47016485 ...  9.7131007  11.35182835\n",
      "  12.71935056]]\n",
      "Running time: 39 seconds\n",
      "\n",
      "Prediction time horizon (delta) = 48.\n",
      "Imputation MAPE: nan\n",
      "Imputation RMSE: nan\n",
      "\n",
      "Prediction MAPE: 2.50218\n",
      "Prediction RMSE: 5.67966\n",
      "\n",
      "[[32.27277252 32.9608335  34.31984881 ...  8.12257549 11.56006114\n",
      "  13.03674819]\n",
      " [ 8.84700067  9.2969742   9.65738896 ...  1.41744648  2.16452117\n",
      "   2.77046745]\n",
      " [ 7.94280908  8.29795687  8.65918862 ...  1.63703504  2.54854907\n",
      "   2.93590502]\n",
      " [ 7.18574139  7.58241499  8.10379574 ...  1.6656732   2.57495773\n",
      "   2.84445671]\n",
      " [ 5.09748787  5.27830128  5.54038076 ...  1.1916498   1.64644806\n",
      "   1.89772087]\n",
      " [30.93141109 31.86687392 33.28425768 ...  6.65650649  9.53931373\n",
      "  11.0889964 ]]\n",
      "Running time: 38 seconds\n",
      "\n",
      "Prediction time horizon (delta) = 96.\n",
      "Imputation MAPE: nan\n",
      "Imputation RMSE: nan\n",
      "\n",
      "Prediction MAPE: 2.03427\n",
      "Prediction RMSE: 6.07377\n",
      "\n",
      "[[32.18578187 33.04348292 34.17352912 ...  0.          2.52286041\n",
      "   5.20936979]\n",
      " [ 8.49202813  8.80389812  9.35181545 ...  0.          0.\n",
      "   0.        ]\n",
      " [ 7.88270654  8.12534846  8.50600795 ...  0.          0.\n",
      "   0.68673533]\n",
      " [ 7.14323978  7.45384314  7.84301316 ...  0.          0.60535973\n",
      "   1.24390833]\n",
      " [ 5.04761714  5.15610927  5.37857208 ...  0.          0.27567175\n",
      "   0.78462274]\n",
      " [30.53898235 31.70893895 32.74672778 ...  0.          0.\n",
      "   2.4283092 ]]\n",
      "Running time: 38 seconds\n",
      "\n",
      "Prediction time horizon (delta) = 672.\n",
      "Imputation MAPE: nan\n",
      "Imputation RMSE: nan\n",
      "\n",
      "Prediction MAPE: 7.30934\n",
      "Prediction RMSE: 9.8064\n",
      "\n",
      "[[3.18687165e+01 3.30344599e+01 3.44709194e+01 ... 1.66099610e+00\n",
      "  3.55577136e-01 0.00000000e+00]\n",
      " [8.37307978e+00 8.67368451e+00 9.07072904e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [7.72898675e+00 8.02610502e+00 8.39567326e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [7.05693415e+00 7.37839852e+00 7.66142624e+00 ... 2.05236115e-01\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [4.92225673e+00 5.05922139e+00 5.24654594e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [3.00313256e+01 3.09801464e+01 3.22001551e+01 ... 3.51217689e-03\n",
      "  0.00000000e+00 0.00000000e+00]]\n",
      "Running time: 37 seconds\n",
      "\n",
      "(6, 3360)\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Assuming loaded_data is already prepared and is in the format of a dense matrix\n",
    "dense_mat = solar_data['uniform_series']  # Adjust this based on how you've prepared loaded_data\n",
    "\n",
    "  # Assuming you want to work on a copy of the original data\n",
    "\n",
    "list_of_arrays = [np.array(series) for series in dense_mat]\n",
    "\n",
    "# Stack these arrays vertically to form a 2D matrix\n",
    "dense_mat_2d = np.vstack(list_of_arrays)\n",
    "sparse_mat = dense_mat_2d.copy()\n",
    "print(\"dense mat shape\",dense_mat_2d.shape)\n",
    "print(dense_mat_2d)\n",
    "dense_mat_2d = np.where(dense_mat_2d == 'NaN', np.nan, dense_mat_2d).astype(float)\n",
    "sparse_mat = np.where(sparse_mat == 'NaN', np.nan, sparse_mat).astype(float)\n",
    "\n",
    "# Model Setting\n",
    "rank = 10\n",
    "pred_step = 2976  # You mentioned you want to predict the next 248 energy demand values\n",
    "time_lags = np.array([1, 4, 96])  # Adjust this based on the seasonality or patterns in your data#\n",
    "burn_iter = 20\n",
    "gibbs_iter = 5\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
    "BTMFburn=20\n",
    "BTMFgibbs=5\n",
    "matrix, W_plus, X_plus, A_plus, Sigma_plus, tau_plus=BTMF(dense_mat_2d, sparse_mat, init, rank, time_lags, BTMFburn, BTMFgibbs, multi_step = 1, option = \"factor\")\n",
    "print(\"matrix shape: \", matrix.shape)\n",
    "print(matrix)\n",
    "df = pd.DataFrame(matrix)  # Assuming mat_hat is your data matrix\n",
    "df.to_csv('C:/Users/Rohit/Documents/Exeter-Placement/Results/solar_predicted_data.csv', index=False)\n",
    "print(\"starting prediction\")\n",
    "# Apply BTMF forecasting for different prediction time horizons (if needed)\n",
    "for multi_step in [4, 24, 48, 96, 672]:  # Adjust this list based on your requirements\n",
    "    start = time.time()\n",
    "    print('Prediction time horizon (delta) = {}.'.format(multi_step))\n",
    "    solar_mat_hat = BTMF_forecast(dense_mat_2d, sparse_mat, pred_step, multi_step, rank, time_lags, burn_iter, gibbs_iter)\n",
    "    print(solar_mat_hat)\n",
    "    end = time.time()\n",
    "    print('Running time: %d seconds'%(end - start))\n",
    "    print()\n",
    "print(solar_mat_hat.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Guangzhou Speed Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $214\\times 61\\times 144$ (road segment, day, time of day)\n",
    "- Test on original data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "tensor = scipy.io.loadmat('../datasets/Guangzhou-data-set/tensor.mat')['tensor']\n",
    "dense_mat = tensor.reshape([tensor.shape[0], tensor.shape[1] * tensor.shape[2]])\n",
    "sparse_mat = dense_mat.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 10\n",
    "- Total (rolling) prediction horizons: 7 * 144\n",
    "- Time lags: {1, 2, 144, 144 + 1, 144 + 2, 7 * 144, 7 * 144 + 1, 7 * 144 + 2}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time horizon (delta) = 2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc23ac2be0c4cbeb04f8e662757b279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=504)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mPrediction time horizon (delta) = \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(multi_step))\n\u001b[1;32m---> 10\u001b[0m mat_hat \u001b[39m=\u001b[39m BTMF_forecast(dense_mat, sparse_mat, pred_step, multi_step, rank, time_lags, burn_iter, gibbs_iter)\n\u001b[0;32m     11\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     12\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mRunning time: \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m seconds\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m(end \u001b[39m-\u001b[39m start))\n",
      "Cell \u001b[1;32mIn[12], line 15\u001b[0m, in \u001b[0;36mBTMF_forecast\u001b[1;34m(dense_mat, sparse_mat, pred_step, multi_step, rank, time_lags, burn_iter, gibbs_iter, option, gamma)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mif\u001b[39;00m t \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     14\u001b[0m     init \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mW\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0.1\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandn(dim1, rank), \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0.1\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandn(start_time, rank)}\n\u001b[1;32m---> 15\u001b[0m     mat, W, X_new, A, Sigma, tau \u001b[39m=\u001b[39m BTMF(dense_mat[:, \u001b[39m0\u001b[39;49m : start_time], \n\u001b[0;32m     16\u001b[0m         sparse_mat[:, \u001b[39m0\u001b[39;49m : start_time], init, rank, time_lags, burn_iter, gibbs_iter, multi_step, option)\n\u001b[0;32m     17\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     18\u001b[0m     init \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mW_plus\u001b[39m\u001b[39m\"\u001b[39m: W, \u001b[39m\"\u001b[39m\u001b[39mX_plus\u001b[39m\u001b[39m\"\u001b[39m: X_new, \u001b[39m\"\u001b[39m\u001b[39mA_plus\u001b[39m\u001b[39m\"\u001b[39m: A, \u001b[39m\"\u001b[39m\u001b[39mSigma_plus\u001b[39m\u001b[39m\"\u001b[39m: Sigma, \u001b[39m\"\u001b[39m\u001b[39mtau_plus\u001b[39m\u001b[39m\"\u001b[39m: tau}\n",
      "Cell \u001b[1;32mIn[9], line 34\u001b[0m, in \u001b[0;36mBTMF\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     32\u001b[0m W \u001b[39m=\u001b[39m sample_factor_w(tau_sparse_mat, tau_ind, W, X, tau, beta0 \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, vargin \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[0;32m     33\u001b[0m A, Sigma \u001b[39m=\u001b[39m sample_var_coefficient(X, time_lags)\n\u001b[1;32m---> 34\u001b[0m X \u001b[39m=\u001b[39m sample_factor_x(tau_sparse_mat, tau_ind, time_lags, W, X, A, inv(Sigma))\n\u001b[0;32m     35\u001b[0m mat_hat \u001b[39m=\u001b[39m W \u001b[39m@\u001b[39m X\u001b[39m.\u001b[39mT\n\u001b[0;32m     36\u001b[0m \u001b[39mif\u001b[39;00m option \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfactor\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "Cell \u001b[1;32mIn[5], line 36\u001b[0m, in \u001b[0;36msample_factor_x\u001b[1;34m(tau_sparse_mat, tau_ind, time_lags, W, X, A, Lambda_x)\u001b[0m\n\u001b[0;32m     34\u001b[0m         temp[:, n] \u001b[39m=\u001b[39m X[t \u001b[39m+\u001b[39m time_lags[k] \u001b[39m-\u001b[39m time_lags, :]\u001b[39m.\u001b[39mreshape(rank \u001b[39m*\u001b[39m d)\n\u001b[0;32m     35\u001b[0m         n \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> 36\u001b[0m     temp0 \u001b[39m=\u001b[39m X[t \u001b[39m+\u001b[39m time_lags[index], :]\u001b[39m.\u001b[39mT \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39;49meinsum(\u001b[39m'\u001b[39;49m\u001b[39mijk, ik -> jk\u001b[39;49m\u001b[39m'\u001b[39;49m, A0[:, :, index], temp)\n\u001b[0;32m     37\u001b[0m     Nt \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39meinsum(\u001b[39m'\u001b[39m\u001b[39mkij, jk -> i\u001b[39m\u001b[39m'\u001b[39m, mat1[index, :, :], temp0)\n\u001b[0;32m     39\u001b[0m var3[:, :, t] \u001b[39m=\u001b[39m var3[:, :, t] \u001b[39m+\u001b[39m Mt\n",
      "File \u001b[1;32mc:\\Users\\Rohit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\einsumfunc.py:1001\u001b[0m, in \u001b[0;36m_einsum_dispatcher\u001b[1;34m(out, optimize, *operands, **kwargs)\u001b[0m\n\u001b[0;32m    997\u001b[0m     path \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39meinsum_path\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m path\n\u001b[0;32m    998\u001b[0m     \u001b[39mreturn\u001b[39;00m (path, path_print)\n\u001b[1;32m-> 1001\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_einsum_dispatcher\u001b[39m(\u001b[39m*\u001b[39moperands, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, optimize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   1002\u001b[0m     \u001b[39m# Arguably we dispatch on more arguments than we really should; see note in\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m     \u001b[39m# _einsum_path_dispatcher for why.\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m     \u001b[39myield from\u001b[39;00m operands\n\u001b[0;32m   1005\u001b[0m     \u001b[39myield\u001b[39;00m out\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "rank = 10\n",
    "pred_step = 7 * 144\n",
    "time_lags = np.array([1, 2, 3, 144, 145, 146, 7 * 144, 7 * 144 + 1, 7 * 144 + 2])\n",
    "burn_iter = 10\n",
    "gibbs_iter = 2\n",
    "for multi_step in [2, 4, 6, 12, 18, 24, 30, 36, 42, 48, 54]:\n",
    "    start = time.time()\n",
    "    print('Prediction time horizon (delta) = {}.'.format(multi_step))\n",
    "    mat_hat = BTMF_forecast(dense_mat, sparse_mat, pred_step, multi_step, rank, time_lags, burn_iter, gibbs_iter)\n",
    "    end = time.time()\n",
    "    print('Running time: %d seconds'%(end - start))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
