{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohitpj/Exeter-Placement/blob/main/transformersNotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "id": "0zPJfSwCP-cy"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "#sys.path.append('/content/drive/MyDrive/Exeter-Placement/BTMF_original.py')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from datetime import datetime\n",
        "from distutils.util import strtobool\n",
        "import sys\n",
        "import BTMF_original\n",
        "import BTMF_Original_Predictor as predictor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import import_ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "id": "BaA4_h2USgB8"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "def convert_tsf_to_dataframe(full_file_path_and_name, replace_missing_vals_with=\"NaN\", value_column_name=\"series_value\",):\n",
        "    col_names = []\n",
        "    col_types = []\n",
        "    all_data = {}\n",
        "    line_count = 0\n",
        "    frequency = None\n",
        "    forecast_horizon = None\n",
        "    contain_missing_values = None\n",
        "    contain_equal_length = None\n",
        "    found_data_tag = False\n",
        "    found_data_section = False\n",
        "    started_reading_data_section = False\n",
        "\n",
        "    with open(full_file_path_and_name, \"r\", encoding=\"cp1252\") as file:\n",
        "        for line in file:\n",
        "            # Strip white space from start/end of line\n",
        "            line = line.strip()\n",
        "\n",
        "            if line:\n",
        "                if line.startswith(\"@\"):  # Read meta-data\n",
        "                    if not line.startswith(\"@data\"):\n",
        "                        line_content = line.split(\" \")\n",
        "                        if line.startswith(\"@attribute\"):\n",
        "                            if (\n",
        "                                len(line_content) != 3\n",
        "                            ):  # Attributes have both name and type\n",
        "                                raise Exception(\"Invalid meta-data specification.\")\n",
        "\n",
        "                            col_names.append(line_content[1])\n",
        "                            col_types.append(line_content[2])\n",
        "                        else:\n",
        "                            if (\n",
        "                                len(line_content) != 2\n",
        "                            ):  # Other meta-data have only values\n",
        "                                raise Exception(\"Invalid meta-data specification.\")\n",
        "\n",
        "                            if line.startswith(\"@frequency\"):\n",
        "                                frequency = line_content[1]\n",
        "                            elif line.startswith(\"@horizon\"):\n",
        "                                forecast_horizon = int(line_content[1])\n",
        "                            elif line.startswith(\"@missing\"):\n",
        "                                contain_missing_values = bool(\n",
        "                                    strtobool(line_content[1])\n",
        "                                )\n",
        "                            elif line.startswith(\"@equallength\"):\n",
        "                                contain_equal_length = bool(strtobool(line_content[1]))\n",
        "\n",
        "                    else:\n",
        "                        if len(col_names) == 0:\n",
        "                            raise Exception(\n",
        "                                \"Missing attribute section. Attribute section must come before data.\"\n",
        "                            )\n",
        "\n",
        "                        found_data_tag = True\n",
        "                elif not line.startswith(\"#\"):\n",
        "                    if len(col_names) == 0:\n",
        "                        raise Exception(\n",
        "                            \"Missing attribute section. Attribute section must come before data.\"\n",
        "                        )\n",
        "                    elif not found_data_tag:\n",
        "                        raise Exception(\"Missing @data tag.\")\n",
        "                    else:\n",
        "                        if not started_reading_data_section:\n",
        "                            started_reading_data_section = True\n",
        "                            found_data_section = True\n",
        "                            all_series = []\n",
        "\n",
        "                            for col in col_names:\n",
        "                                all_data[col] = []\n",
        "\n",
        "                        full_info = line.split(\":\")\n",
        "\n",
        "                        if len(full_info) != (len(col_names) + 1):\n",
        "                            raise Exception(\"Missing attributes/values in series.\")\n",
        "\n",
        "                        series = full_info[len(full_info) - 1]\n",
        "                        series = series.split(\",\")\n",
        "\n",
        "                        if len(series) == 0:\n",
        "                            raise Exception(\n",
        "                                \"A given series should contains a set of comma separated numeric values. At least one numeric value should be there in a series. Missing values should be indicated with ? symbol\"\n",
        "                            )\n",
        "\n",
        "                        numeric_series = []\n",
        "\n",
        "                        for val in series:\n",
        "                            if val == \"?\":\n",
        "                                numeric_series.append(replace_missing_vals_with)\n",
        "                            else:\n",
        "                                numeric_series.append(float(val))\n",
        "\n",
        "                        if numeric_series.count(replace_missing_vals_with) == len(\n",
        "                            numeric_series\n",
        "                        ):\n",
        "                            raise Exception(\n",
        "                                \"All series values are missing. A given series should contains a set of comma separated numeric values. At least one numeric value should be there in a series.\"\n",
        "                            )\n",
        "\n",
        "                        all_series.append(pd.Series(numeric_series).array)\n",
        "\n",
        "                        for i in range(len(col_names)):\n",
        "                            att_val = None\n",
        "                            if col_types[i] == \"numeric\":\n",
        "                                att_val = int(full_info[i])\n",
        "                            elif col_types[i] == \"string\":\n",
        "                                att_val = str(full_info[i])\n",
        "                            elif col_types[i] == \"date\":\n",
        "                                att_val = datetime.strptime(\n",
        "                                    full_info[i], \"%Y-%m-%d %H-%M-%S\"\n",
        "                                )\n",
        "                            else:\n",
        "                                raise Exception(\n",
        "                                    \"Invalid attribute type.\"\n",
        "                                )  # Currently, the code supports only numeric, string and date types. Extend this as required.\n",
        "\n",
        "                            if att_val is None:\n",
        "                                raise Exception(\"Invalid attribute value.\")\n",
        "                            else:\n",
        "                                all_data[col_names[i]].append(att_val)\n",
        "\n",
        "                line_count = line_count + 1\n",
        "\n",
        "        if line_count == 0:\n",
        "            raise Exception(\"Empty file.\")\n",
        "        if len(col_names) == 0:\n",
        "            raise Exception(\"Missing attribute section.\")\n",
        "        if not found_data_section:\n",
        "            raise Exception(\"Missing series information under data section.\")\n",
        "\n",
        "        all_data[value_column_name] = all_series\n",
        "        loaded_data = pd.DataFrame(all_data)\n",
        "\n",
        "        return (\n",
        "            loaded_data,\n",
        "            frequency,\n",
        "            forecast_horizon,\n",
        "            contain_missing_values,\n",
        "            contain_equal_length,\n",
        "        )\n",
        "# Example of usage\n",
        "# loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(\"TSForecasting/tsf_data/sample.tsf\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {
        "id": "XNtA88ETUVQ0"
      },
      "outputs": [],
      "source": [
        "def trim_dataframe(filename,columnname):\n",
        "    loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(filename)\n",
        "    building_data = loaded_data[loaded_data['series_name'].str.contains(columnname)]\n",
        "    max_start_timestamp = building_data['start_timestamp'].max()\n",
        "    building_data['num_timestamps'] = building_data['series_value'].apply(len)\n",
        "    min_timestamps = building_data['num_timestamps'].min()\n",
        "    building_data['uniform_series'] = building_data['series_value'].apply(lambda x: x[-min_timestamps:])\n",
        "    building_data['num_timestamps'] = building_data['uniform_series'].apply(len)\n",
        "    return building_data['uniform_series']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {
        "id": "y-YamZO0UXMn"
      },
      "outputs": [],
      "source": [
        "def impute_dataframe(dataframe, rank, time_lags, burn_iter, gibbs_iter, option = \"factor\"):\n",
        "    if isinstance(dataframe, pd.Series):\n",
        "        dense_mat_2d = dataframe.values.reshape(-1, 1)  # Convert series to 2D array\n",
        "    elif isinstance(dataframe, pd.DataFrame):\n",
        "        list_of_arrays = [np.array(series) for series in dataframe]\n",
        "        dense_mat_2d = np.vstack(list_of_arrays)\n",
        "    else:\n",
        "        raise ValueError(\"Input data should be either a pandas DataFrame or Series.\")\n",
        "\n",
        "    \n",
        "    dense_tensor = dataframe\n",
        "    dim = dense_tensor.shape\n",
        "    list_of_arrays = [np.array(series) for series in dense_tensor]\n",
        "    # Stack these arrays vertically to form a 2D matrix\n",
        "    dense_mat_2d = np.vstack(list_of_arrays)\n",
        "    sparse_mat = dense_mat_2d.copy()\n",
        "    dense_mat_2d = np.where(dense_mat_2d == 'NaN', np.nan, dense_mat_2d).astype(float)\n",
        "    sparse_mat = np.where(sparse_mat == 'NaN', np.nan, sparse_mat).astype(float)\n",
        "    del dense_tensor\n",
        "    dim1, dim2 = sparse_mat.shape\n",
        "    init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
        "    mat, W, X, A= BTMF_original.BTMF(dense_mat_2d, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter)\n",
        "    # Assuming you have column names and indices stored\n",
        "    df=pd.DataFrame(mat)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ubX1fu3UaW1",
        "outputId": "fa7f80bb-bc6d-4b06-db5f-a42f831c40c1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Rohit\\AppData\\Local\\Temp\\ipykernel_7732\\423370617.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  building_data['num_timestamps'] = building_data['series_value'].apply(len)\n",
            "C:\\Users\\Rohit\\AppData\\Local\\Temp\\ipykernel_7732\\423370617.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  building_data['uniform_series'] = building_data['series_value'].apply(lambda x: x[-min_timestamps:])\n",
            "C:\\Users\\Rohit\\AppData\\Local\\Temp\\ipykernel_7732\\423370617.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  building_data['num_timestamps'] = building_data['uniform_series'].apply(len)\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'Series' object has no attribute 'type'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7732\\838622026.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mNaN_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/Rohit/Documents/Exeter-Placement/Challenge/phase_1 data/phase_1_data/phase_1_data.tsf\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Building\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNaN_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtime_lags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m96\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7732\\423370617.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(filename, columnname)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mbuilding_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'num_timestamps'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuilding_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'series_value'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mmin_timestamps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuilding_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'num_timestamps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mbuilding_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'uniform_series'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuilding_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'series_value'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mmin_timestamps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mbuilding_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'num_timestamps'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuilding_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'uniform_series'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuilding_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'uniform_series'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbuilding_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'uniform_series'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Rohit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5985\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5986\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5987\u001b[0m         ):\n\u001b[0;32m   5988\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5989\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'type'"
          ]
        }
      ],
      "source": [
        "NaN_df=trim_dataframe(\"C:/Users/Rohit/Documents/Exeter-Placement/Challenge/phase_1 data/phase_1_data/phase_1_data.tsf\",\"Building\")\n",
        "print(type(NaN_df))\n",
        "\n",
        "time_lags = np.array([1, 4, 96])\n",
        "burn_iter=0\n",
        "gibbs_iter=1\n",
        "rank=10\n",
        "\n",
        "df=impute_dataframe(NaN_df,rank,time_lags,burn_iter,gibbs_iter)\n",
        "scaler = MinMaxScaler()\n",
        "df_scaled = pd.DataFrame(scaler.fit_transform(df.T).T, columns=df.columns, index=df.index)\n",
        "\n",
        "sequence_length = 5760  # 3 months of data\n",
        "\n",
        "#sequence_length = 8640  # 3 months of data\n",
        "prediction_length = 2880  # 1 month of data\n",
        "\n",
        "train_end = 41572 - prediction_length - sequence_length\n",
        "train_data = df_scaled.iloc[:, :train_end]\n",
        "validation_data = df_scaled.iloc[:, train_end:train_end+sequence_length]\n",
        "test_data = df_scaled.iloc[:, train_end+sequence_length:train_end+2*sequence_length]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y52n-XhaUc06"
      },
      "outputs": [],
      "source": [
        "def create_sequences(data, seq_length, pred_length):\n",
        "    print(\"sequence_length, pred_length, sequence_length + pred_length\")\n",
        "    print(sequence_length, pred_length, sequence_length + pred_length)\n",
        "\n",
        "    sequences = []\n",
        "    target_sequences = []\n",
        "\n",
        "    for i in range(len(data.columns) - seq_length - pred_length + 1):\n",
        "        sequences.append(data.iloc[:, i:i+seq_length].values)\n",
        "        target_sequences.append(data.iloc[:, i+seq_length:i+seq_length+pred_length].values)\n",
        "\n",
        "    return np.array(sequences), np.array(target_sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "id": "MNtl0uZ4Ud6Q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sequence_length, pred_length, sequence_length + pred_length\n",
            "5760 2880 8640\n",
            "sequence_length, pred_length, sequence_length + pred_length\n",
            "5760 2880 8640\n"
          ]
        }
      ],
      "source": [
        "X_train, Y_train = create_sequences(train_data, sequence_length, prediction_length)\n",
        "X_val, Y_val = create_sequences(validation_data, sequence_length, prediction_length)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "id": "VVv08J8AUgET"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(seq_length, d_model):\n",
        "    position = np.arange(seq_length)[:, np.newaxis]\n",
        "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
        "    pos_enc = np.zeros((d_model, seq_length))\n",
        "    pos_enc[0::2, :] = np.sin(position * div_term).T\n",
        "    pos_enc[1::2, :] = np.cos(position * div_term).T\n",
        "    return pos_enc[np.newaxis, :, :]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzduyfWeUgkh",
        "outputId": "b17c9cd9-d341-4117-bad6-849c91c55e31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 6, 5760) <class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "d_model = 6  # Number of sensors\n",
        "# Generate positional encoding\n",
        "pos_enc = positional_encoding(sequence_length, d_model)\n",
        "print(pos_enc.shape, type(pos_enc))\n",
        "# Add positional encoding to X_train\n",
        "X_val = validation_data.values[np.newaxis, :, :]\n",
        "X_train += pos_enc\n",
        "X_val += pos_enc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {
        "id": "3AMz8PHvU2sA"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        assert d_model % self.num_heads == 0\n",
        "        self.depth = d_model // self.num_heads\n",
        "        \"\"\"\n",
        "        self.wq = nn.Linear(8640, d_model)\n",
        "        self.wk = nn.Linear(8640, d_model)\n",
        "        self.wv = nn.Linear(8640, d_model)\n",
        "        \"\"\"\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, d_model)\n",
        "        self.wv = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dense = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, v, k, q, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        q = self.split_heads(self.wq(q), batch_size)\n",
        "        k = self.split_heads(self.wk(k), batch_size)\n",
        "        v = self.split_heads(self.wv(v), batch_size)\n",
        "\n",
        "        matmul_qk = torch.matmul(q, k.permute(0, 1, 3, 2))\n",
        "        d_k = self.depth ** 0.5\n",
        "        scaled_attention_logits = matmul_qk / d_k\n",
        "\n",
        "        if mask is not None:\n",
        "            scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "        attention_weights = F.softmax(scaled_attention_logits, dim=-1)\n",
        "        output = torch.matmul(attention_weights, v)\n",
        "\n",
        "        output = output.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
        "        return self.dense(output), attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "id": "d19EaOAcVJdI"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = nn.Sequential(\n",
        "          nn.Linear(d_model, dff),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(dff, d_model)\n",
        "        )\n",
        "\n",
        "        self.layernorm1 = nn.LayerNorm(d_model)\n",
        "        self.layernorm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "id": "_y5fZmsoVMy6"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, max_position_encoding, dropout_rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.input_proj = nn.Linear(8640, 512)\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Create a positional encoding tensor with a sequence length of 8640\n",
        "        #self.pos_encoding = torch.tensor(positional_encoding(8640, 512), dtype=torch.float32)\n",
        "\n",
        "        self.enc_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, dff, dropout_rate) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, training, mask=None):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        #print(\"batch size\",batch_size,\"seq length\",seq_len)\n",
        "        #print(f\"X Initial shape: {x.shape}\")\n",
        "        \n",
        "        # Reshape and apply input_proj\n",
        "        x = x.view(-1, 8640)\n",
        "        #print(f\"X After first reshape: {x.shape}\")\n",
        "        x = self.input_proj(x)\n",
        "        #print(f\"X After input_proj: {x.shape}\")\n",
        "        x = x.view(batch_size, seq_len, -1)  # Use the model dimension for reshaping\n",
        "        #print(f\"X After second reshape: {x.shape}\")\n",
        "\n",
        "        #self.pos_encoding = torch.tensor(positional_encoding(seq_len, self.d_model), dtype=torch.float32)\n",
        "        self.pos_encoding = torch.tensor(positional_encoding(self.d_model,seq_len), dtype=torch.float32)\n",
        "\n",
        "        #print(\"  pos encoding shape\",self.pos_encoding.shape)\n",
        "        self.pos_encoding = self.pos_encoding.clone().detach().to(x.device)\n",
        "        #print(\"  pos encoding shape after dupe\",self.pos_encoding.shape)       \n",
        "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
        "\n",
        "        # Print shapes before adding positional encoding\n",
        "        #print(f\"Shape of x: {x.shape}\")\n",
        "        #print(f\"Shape of positional encoding: {self.pos_encoding.shape}\")\n",
        "\n",
        "        # Add positional encoding\n",
        "        #add=self.pos_encoding[:, :seq_len, :]\n",
        "        add=self.pos_encoding\n",
        "        \n",
        "        #print(\"add shape\", add.shape)\n",
        "        x += add\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        #print(\"applying input projection\")\n",
        "        #print(\"final x shape\", x.shape)\n",
        "        for i in range(self.num_layers):\n",
        "            #print(f\"Properties of EncoderLayer {i}:\")\n",
        "            for name, param in self.enc_layers[i].named_parameters():\n",
        "                pass\n",
        "                #print(f\"Parameter Name: {name}, Shape: {param.shape}\")\n",
        "            \n",
        "            x = self.enc_layers[i](x, mask)\n",
        "            #print(\" layer\",i,\"applied\")\n",
        "            #print(\"X after encoder layer \",i, \" \", x.shape)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {
        "id": "A3nin5F6V7_0"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, dff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dff, d_model)\n",
        "        )\n",
        "\n",
        "        self.layernorm1 = nn.LayerNorm(d_model)\n",
        "        self.layernorm2 = nn.LayerNorm(d_model)\n",
        "        self.layernorm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "        self.dropout3 = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, enc_output, training, look_ahead_mask=None, padding_mask=None):\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1)  # Remove training=training\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
        "        attn2 = self.dropout2(attn2)  # Remove training=training\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output)  # Remove training=training\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {
        "id": "oM2quFbkV9vw"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, max_position_encoding, dropout_rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.input_proj = nn.Linear(2880, 512)\n",
        "        self.output_proj = nn.Linear(512, 2880)  # Add this line\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.pos_encoding = torch.tensor(positional_encoding(max_position_encoding, d_model), dtype=torch.float32)\n",
        "\n",
        "        self.dec_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, dff, dropout_rate) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, enc_output, look_ahead_mask=None, padding_mask=None):\n",
        "        seq_len = x.size(1)\n",
        "        attention_weights = {}\n",
        "        #print(f\"Decoder X Initial shape: {x.shape}\")\n",
        "\n",
        "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
        "        x = self.input_proj(x)\n",
        "\n",
        "        x += self.pos_encoding[:, :seq_len, :x.size(2)]\n",
        "        #print(f\"Decoder X after pos encoding: {x.shape}\")\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, look_ahead_mask, padding_mask)\n",
        "            #print(\"X after decoder layer \",i, \" \", x.shape)\n",
        "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "        x = self.output_proj(x)  # Add this line\n",
        "\n",
        "        return x, attention_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {
        "id": "UAr55WK-V_kv"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, dropout_rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, dropout_rate)\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, dropout_rate)\n",
        "\n",
        "        #self.final_layer = nn.Linear(d_model, target_vocab_size)\n",
        "        self.final_layer = nn.Linear(512, 2880)\n",
        "\n",
        "        #self.final_layer = nn.Linear(2880, target_vocab_size)\n",
        "    def forward(self, inp, tar, training=True, enc_padding_mask=None, look_ahead_mask=None, dec_padding_mask=None):\n",
        "        enc_output = self.encoder(inp, enc_padding_mask)\n",
        "        #dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "        dec_output, attention_weights = self.decoder(tar, enc_output, look_ahead_mask, dec_padding_mask)\n",
        "        #print(\"Shape of dec_output:\", dec_output.shape)\n",
        "        final_output = dec_output\n",
        "        #final_output = self.final_layer(dec_output)\n",
        "        #print(\"final output shape: \", final_output.shape)\n",
        "        return final_output, attention_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "id": "CHNB_7c3Wqwf"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "input_vocab_size = 512\n",
        "target_vocab_size = 512\n",
        "max_position_encoding_input = 9000  # A bit more than 8640 to ensure coverage\n",
        "max_position_encoding_target = 9000\n",
        "\n",
        "# Create the Transformer model instance\n",
        "model = Transformer(num_layers=6, d_model=512, num_heads=8, dff=2048,\n",
        "                    input_vocab_size=input_vocab_size, target_vocab_size=target_vocab_size,\n",
        "                    pe_input=max_position_encoding_input, pe_target=max_position_encoding_target)\n",
        "model.train()\n",
        "# Now, you can define your optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {
        "id": "Y-7Az3tkWIK1"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with a learning rate of 0.001\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "id": "_3ofdPV0b1fW"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.Y[idx]\n",
        "\n",
        "train_dataset = TimeSeriesDataset(X_train, Y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "id": "wbGk5J7Gb-Dv"
      },
      "outputs": [],
      "source": [
        "model = Transformer(num_layers=6, d_model=512, num_heads=8, dff=2048,\n",
        "                    input_vocab_size=512, target_vocab_size=512,\n",
        "                    pe_input=9000, pe_target=9000)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {
        "id": "7_HkKUhuWIz3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<enumerate object at 0x00000147BFB5EFC0>\n",
            "batch num  0\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "shape '[-1, 8640]' is invalid for input of size 633600",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[236], line 16\u001b[0m\n\u001b[0;32m     12\u001b[0m target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m     14\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m# Exclude the last time step from target as input to the decoder\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m outputs, _ \u001b[39m=\u001b[39m model(data, target[:, :\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\n\u001b[0;32m     17\u001b[0m \u001b[39m#print(\"computing loss\")\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m# Compute the loss\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m# Exclude the first time step from target to match the shape of outputs\u001b[39;00m\n\u001b[0;32m     20\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, target[:, \u001b[39m1\u001b[39m:])\n",
            "File \u001b[1;32mc:\\Users\\Rohit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[1;32mIn[221], line 13\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inp, tar, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, enc_padding_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, look_ahead_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dec_padding_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m---> 13\u001b[0m     enc_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(inp, enc_padding_mask)\n\u001b[0;32m     14\u001b[0m     \u001b[39m#dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     dec_output, attention_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(tar, enc_output, look_ahead_mask, dec_padding_mask)\n",
            "File \u001b[1;32mc:\\Users\\Rohit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[1;32mIn[218], line 20\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, x, training, mask)\u001b[0m\n\u001b[0;32m     15\u001b[0m batch_size, seq_len, _ \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msize()\n\u001b[0;32m     16\u001b[0m \u001b[39m#print(\"batch size\",batch_size,\"seq length\",seq_len)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m#print(f\"X Initial shape: {x.shape}\")\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m \u001b[39m# Reshape and apply input_proj\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m8640\u001b[39;49m)\n\u001b[0;32m     21\u001b[0m \u001b[39m#print(f\"X After first reshape: {x.shape}\")\u001b[39;00m\n\u001b[0;32m     22\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_proj(x)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: shape '[-1, 8640]' is invalid for input of size 633600"
          ]
        }
      ],
      "source": [
        "num_epochs = 1\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    print(enumerate(train_loader))\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        if batch_idx %50==0:\n",
        "            print(\"batch num \", batch_idx)\n",
        "        optimizer.zero_grad()\n",
        "        data = data.float()\n",
        "        #print(\"Input data shape:\", data.shape)\n",
        "        target = target.float()\n",
        "\n",
        "        # Forward pass\n",
        "        # Exclude the last time step from target as input to the decoder\n",
        "        outputs, _ = model(data, target[:, :-1])\n",
        "        #print(\"computing loss\")\n",
        "        # Compute the loss\n",
        "        # Exclude the first time step from target to match the shape of outputs\n",
        "        loss = criterion(outputs, target[:, 1:])\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        #print(\"backward pass\")\n",
        "        loss.backward()\n",
        "        #print(\"optimization\")\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    print(\"total loss: \", total_loss)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {
        "id": "S3k8vaospFic"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Rohit\\AppData\\Local\\Temp\\ipykernel_7732\\1322639957.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  building_data['num_timestamps'] = building_data['series_value'].apply(len)\n",
            "C:\\Users\\Rohit\\AppData\\Local\\Temp\\ipykernel_7732\\1322639957.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  building_data['uniform_series'] = building_data['series_value'].apply(lambda x: x[-min_timestamps:])\n",
            "C:\\Users\\Rohit\\AppData\\Local\\Temp\\ipykernel_7732\\1322639957.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  building_data['num_timestamps'] = building_data['uniform_series'].apply(len)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    [NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, ...\n",
            "1    [26.6, 6.2, 12.7, 19.8, 26.0, 7.0, 14.3, 20.7,...\n",
            "2    [690.0, 795.0, 795.0, 795.0, 795.0, 867.0, 867...\n",
            "3    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, NaN, ...\n",
            "4    [30.0, 31.0, 24.0, 34.0, 30.0, 31.0, 26.0, 33....\n",
            "5    [40.0, 40.0, 35.6, 35.6, 37.6, 37.6, 37.4, 37....\n",
            "Name: uniform_series, dtype: object\n",
            "Sensor 0: 41572 data points\n",
            "Sensor 1: 41572 data points\n",
            "Sensor 2: 41572 data points\n",
            "Sensor 3: 41572 data points\n",
            "Sensor 4: 41572 data points\n",
            "Sensor 5: 41572 data points\n",
            "NaN type <class 'pandas.core.series.Series'>\n",
            "4\n",
            "4\n",
            "BTMF Iteration: 0\n",
            "BTMF Iteration: 1\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[232], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m rank \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m     51\u001b[0m \u001b[39m# Apply BTMF imputation to each dataframe\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m train_imputed \u001b[39m=\u001b[39m impute_dataframe(train_series, rank, time_lags, burn_iter, gibbs_iter)\n\u001b[0;32m     53\u001b[0m val_imputed \u001b[39m=\u001b[39m impute_dataframe(val_series, rank, time_lags, burn_iter, gibbs_iter)\n\u001b[0;32m     54\u001b[0m test_imputed \u001b[39m=\u001b[39m impute_dataframe(test_series, rank, time_lags, burn_iter, gibbs_iter)\n",
            "Cell \u001b[1;32mIn[210], line 22\u001b[0m, in \u001b[0;36mimpute_dataframe\u001b[1;34m(dataframe, rank, time_lags, burn_iter, gibbs_iter, option)\u001b[0m\n\u001b[0;32m     20\u001b[0m dim1, dim2 \u001b[39m=\u001b[39m sparse_mat\u001b[39m.\u001b[39mshape\n\u001b[0;32m     21\u001b[0m init \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mW\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0.1\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandn(dim1, rank), \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0.1\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandn(dim2, rank)}\n\u001b[1;32m---> 22\u001b[0m mat, W, X, A\u001b[39m=\u001b[39m BTMF_original\u001b[39m.\u001b[39;49mBTMF(dense_mat_2d, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter)\n\u001b[0;32m     23\u001b[0m \u001b[39m# Assuming you have column names and indices stored\u001b[39;00m\n\u001b[0;32m     24\u001b[0m df\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39mDataFrame(mat)\n",
            "File \u001b[1;32mc:\\Users\\Rohit\\Documents\\Exeter-Placement\\BTMF_original.py:178\u001b[0m, in \u001b[0;36mBTMF\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_rmse\u001b[39m(var, var_hat):\n\u001b[0;32m    148\u001b[0m     \u001b[39mreturn\u001b[39;00m  np\u001b[39m.\u001b[39msqrt(np\u001b[39m.\u001b[39msum((var \u001b[39m-\u001b[39m var_hat) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m) \u001b[39m/\u001b[39m var\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[0;32m    150\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[39mdef BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter, option = \"factor\"):\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[39m    original_mat = sparse_mat.copy()\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[39m    dim1, dim2 = sparse_mat.shape\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[39m    #print(\"dense_mat shape\",dense_mat.shape)\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[39m    d = time_lags.shape[0]\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[39m    W = init[\"W\"]\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[39m    X = init[\"X\"]\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[0;32m    159\u001b[0m \u001b[39m    if np.isnan(sparse_mat).any() == False:\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[39m        ind = sparse_mat != 0\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[39m        pos_obs = np.where(ind)\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39m        pos_test = np.where((dense_mat != 0) & (sparse_mat == 0))\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \n\u001b[0;32m    164\u001b[0m \u001b[39m        #print(\"pos test  shape: \", pos_test)\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[39m    elif np.isnan(sparse_mat).any() == True:\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[39m        pos_test = np.where((dense_mat != 0) & (np.isnan(sparse_mat)))\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39m        ind = ~np.isnan(sparse_mat)\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[39m        pos_obs = np.where(ind)\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[39m        sparse_mat[np.isnan(sparse_mat)] = 0\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \n\u001b[0;32m    171\u001b[0m \u001b[39m    dense_test = dense_mat[pos_test]\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m    del dense_mat\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[39m    tau = np.ones(dim1)\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[39m    W_plus = np.zeros((dim1, rank))\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[39m    X_plus = np.zeros((dim2, rank))\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[39m    A_plus = np.zeros((rank * d, rank))\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[39m    temp_hat = np.zeros(len(pos_test[0]))\u001b[39;00m\n\u001b[1;32m--> 178\u001b[0m \u001b[39m    show_iter = 999\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[39m    mat_hat_plus = np.zeros((dim1, dim2))\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[39m    for it in range(burn_iter + gibbs_iter):\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[39m        print(\"BTMF Iteration:\",it)\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[39m        tau_ind = tau[:, None] * ind\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[39m        tau_sparse_mat = tau[:, None] * sparse_mat\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[39m        W = sample_factor_w(tau_sparse_mat, tau_ind, W, X, tau)\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[39m        A, Sigma = sample_var_coefficient(X, time_lags)\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[39m        X = sample_factor_x(tau_sparse_mat, tau_ind, time_lags, W, X, A, inv(Sigma))\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[39m        mat_hat = W @ X.T\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[39m        if option == \"factor\":\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[39m            tau = sample_precision_tau(sparse_mat, mat_hat, ind)\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[39m        elif option == \"pca\":\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[39m            tau = sample_precision_scalar_tau(sparse_mat, mat_hat, ind)\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[39m            tau = tau * np.ones(dim1)\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[39m        temp_hat += mat_hat[pos_test]\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[39m        if (it + 1) % show_iter == 0 and it < burn_iter:\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m            temp_hat = temp_hat / show_iter\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m            print('Iter: {}'.format(it + 1))\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[39m            print('MAPE: {:.6}'.format(compute_mape(dense_test, temp_hat)))\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m            print('RMSE: {:.6}'.format(compute_rmse(dense_test, temp_hat)))\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m            temp_hat = np.zeros(len(pos_test[0]))\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[39m            print()\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[39m        if it + 1 > burn_iter:\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[39m            W_plus += W\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[39m            X_plus += X\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39m            A_plus += A\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[39m            mat_hat_plus += mat_hat\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[39m    mat_hat = mat_hat_plus / gibbs_iter\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39m    W = W_plus / gibbs_iter\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[39m    X = X_plus / gibbs_iter\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \u001b[39m    A = A_plus / gibbs_iter\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[39m    #print('Imputation MAPE: {:.6}'.format(compute_mape(dense_test, mat_hat[:, : dim2][pos_test])))\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[39m    #print('Imputation RMSE: {:.6}'.format(compute_rmse(dense_test, mat_hat[:, : dim2][pos_test])))\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[39m    #print()\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[39m    mat_hat[mat_hat < 0] = 0\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[39m    nan_positions = np.isnan(original_mat)\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[39m    original_mat[nan_positions] = mat_hat[nan_positions]\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \u001b[39m    return original_mat, W, X, A\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mBTMF\u001b[39m(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter, multi_step \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, option \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfactor\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    219\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Bayesian Temporal Matrix Factorization, BTMF.\"\"\"\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Rohit\\Documents\\Exeter-Placement\\BTMF_original.py:114\u001b[0m, in \u001b[0;36msample_factor_x\u001b[1;34m(tau_sparse_mat, tau_ind, time_lags, W, X, A, Lambda_x)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39mif\u001b[39;00m t \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m dim2 \u001b[39m-\u001b[39m tmax \u001b[39mand\u001b[39;00m t \u001b[39m<\u001b[39m dim2 \u001b[39m-\u001b[39m tmin:\n\u001b[0;32m    113\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(np\u001b[39m.\u001b[39mwhere(t \u001b[39m+\u001b[39m time_lags \u001b[39m<\u001b[39m dim2))[\u001b[39m0\u001b[39m]\n\u001b[1;32m--> 114\u001b[0m \u001b[39melif\u001b[39;00m t \u001b[39m<\u001b[39m tmax:\n\u001b[0;32m    115\u001b[0m     Qt \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(rank)\n\u001b[0;32m    116\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(np\u001b[39m.\u001b[39mwhere(t \u001b[39m+\u001b[39m time_lags \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m tmax))[\u001b[39m0\u001b[39m]\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "NaN_df=trim_dataframe(\"C:/Users/Rohit/Documents/Exeter-Placement/Challenge/phase_1 data/phase_1_data/phase_1_data.tsf\",\"Building\")\n",
        "#41572 data points in each sensor\n",
        "\n",
        "total_data_points = len(NaN_df)\n",
        "print(\"NaN type\",type(NaN_df))\n",
        "print(train_end)\n",
        "print(val_end)\n",
        "# Define split ratios\n",
        "train_ratio = 0.7\n",
        "val_ratio = 0.3\n",
        "# test_ratio is implicitly defined as 1 - train_ratio - val_ratio\n",
        "\n",
        "# Calculate the number of data points for each set\n",
        "train_end = int(train_ratio * total_data_points)\n",
        "val_end = train_end + int(val_ratio * total_data_points)\n",
        "\n",
        "# Split the dataframe\n",
        "def split_sensor_data(sensor_data, train_ratio, val_ratio):\n",
        "    total_points = len(sensor_data)\n",
        "    train_end = int(train_ratio * total_points)\n",
        "    val_end = train_end + int(val_ratio * total_points)\n",
        "    \n",
        "    return sensor_data[:train_end], sensor_data[train_end:val_end], sensor_data[val_end:]\n",
        "\n",
        "train_data = []\n",
        "val_data = []\n",
        "test_data = []\n",
        "\n",
        "for sensor_readings in NaN_df:\n",
        "    train, val, test = split_sensor_data(sensor_readings, train_ratio, val_ratio)\n",
        "    train_data.append(train)\n",
        "    val_data.append(val)\n",
        "    test_data.append(test)\n",
        "\n",
        "# Convert lists back to Series or DataFrame if needed\n",
        "train_series = pd.Series(train_data)\n",
        "val_series = pd.Series(val_data)\n",
        "test_series = pd.Series(test_data)\n",
        "#2 weeks\n",
        "pred_length = 1344\n",
        "#pred_length = 2976\n",
        "time_lags = np.array([1, 4, 96])\n",
        "burn_iter = 5\n",
        "gibbs_iter = 2\n",
        "rank = 10\n",
        "\n",
        "# Apply BTMF imputation to each dataframe\n",
        "train_imputed = impute_dataframe(train_series, rank, time_lags, burn_iter, gibbs_iter)\n",
        "val_imputed = impute_dataframe(val_series, rank, time_lags, burn_iter, gibbs_iter)\n",
        "test_imputed = impute_dataframe(test_series, rank, time_lags, burn_iter, gibbs_iter)\n",
        "\n",
        "print(\"Training data after imputation:\", train_imputed.shape)\n",
        "print(\"Validation data after imputation:\", val_imputed.shape)\n",
        "print(\"Test data after imputation:\", test_imputed.shape)\n",
        "\n",
        "# Scale the imputed data\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "train_scaled = pd.DataFrame(scaler.fit_transform(train_imputed.T).T, columns=train_imputed.columns, index=train_imputed.index)\n",
        "val_scaled = pd.DataFrame(scaler.transform(val_imputed.T).T, columns=val_imputed.columns, index=val_imputed.index)\n",
        "test_scaled = pd.DataFrame(scaler.transform(test_imputed.T).T, columns=test_imputed.columns, index=test_imputed.index)\n",
        "print(\"val scaled len\",val_scaled.shape)\n",
        "\n",
        "sequence_length = 3300  # 3 months of data\n",
        "prediction_length = 2880  # 1 month of data\n",
        "\n",
        "train_end = 41572 - prediction_length - sequence_length\n",
        "print(\"train end\",train_end)\n",
        "\n",
        "train_data = df_scaled.iloc[:, :train_end]\n",
        "validation_data = df_scaled.iloc[:, train_end:train_end+sequence_length]\n",
        "test_data = df_scaled.iloc[:, train_end+sequence_length:train_end+2*sequence_length]\n",
        "\n",
        "train_sequences, train_labels = create_sequences(train_scaled, sequence_length, pred_length)\n",
        "val_sequences, val_labels = create_sequences(val_scaled, sequence_length, pred_length)\n",
        "\n",
        "print(\"val seq len\",len(val_sequences))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd5pXQdZpvkv"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "shape '[-1, 8640]' is invalid for input of size 633600",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[228], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     36\u001b[0m data, target \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mfloat(), target\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m---> 38\u001b[0m outputs, _ \u001b[39m=\u001b[39m model(data, target[:, :\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\n\u001b[0;32m     39\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, target[:, \u001b[39m1\u001b[39m:])\n\u001b[0;32m     41\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
            "File \u001b[1;32mc:\\Users\\Rohit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[1;32mIn[221], line 13\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inp, tar, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, enc_padding_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, look_ahead_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dec_padding_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m---> 13\u001b[0m     enc_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(inp, enc_padding_mask)\n\u001b[0;32m     14\u001b[0m     \u001b[39m#dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     dec_output, attention_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(tar, enc_output, look_ahead_mask, dec_padding_mask)\n",
            "File \u001b[1;32mc:\\Users\\Rohit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[1;32mIn[218], line 20\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, x, training, mask)\u001b[0m\n\u001b[0;32m     15\u001b[0m batch_size, seq_len, _ \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msize()\n\u001b[0;32m     16\u001b[0m \u001b[39m#print(\"batch size\",batch_size,\"seq length\",seq_len)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m#print(f\"X Initial shape: {x.shape}\")\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m \u001b[39m# Reshape and apply input_proj\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m8640\u001b[39;49m)\n\u001b[0;32m     21\u001b[0m \u001b[39m#print(f\"X After first reshape: {x.shape}\")\u001b[39;00m\n\u001b[0;32m     22\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_proj(x)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: shape '[-1, 8640]' is invalid for input of size 633600"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the dataset class\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.Y[idx]\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 32\n",
        "train_dataset = TimeSeriesDataset(train_sequences, train_labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_dataset = TimeSeriesDataset(val_sequences, val_labels)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 1  # Adjust as needed\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        if batch_idx %50==0:\n",
        "            print(batch_idx)\n",
        "        optimizer.zero_grad()\n",
        "        data, target = data.float(), target.float()\n",
        "        \n",
        "        outputs, _ = model(data, target[:, :-1])\n",
        "        loss = criterion(outputs, target[:, 1:])\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            print(\"a\")\n",
        "            data, target = data.float(), target.float()\n",
        "            \n",
        "            outputs, _ = model(data, target[:, :-1])\n",
        "            loss = criterion(outputs, target[:, 1:])\n",
        "            \n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    # Print epoch results\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Save the best model\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), 'best_transformer_model.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOiPQZglxoKZu7/sIBv+Lop",
      "include_colab_link": true,
      "mount_file_id": "1fP-ZxX6dalSO3A7vKNed7Co6n_zBHeqA",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
