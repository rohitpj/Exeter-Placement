{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1fP-ZxX6dalSO3A7vKNed7Co6n_zBHeqA",
      "authorship_tag": "ABX9TyOiPQZglxoKZu7/sIBv+Lop",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohitpj/Exeter-Placement/blob/main/transformersNotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0zPJfSwCP-cy"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Exeter-Placement/BTMF_original.py')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from datetime import datetime\n",
        "from distutils.util import strtobool\n",
        "import sys\n",
        "import BTMF_original\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "def convert_tsf_to_dataframe(full_file_path_and_name, replace_missing_vals_with=\"NaN\", value_column_name=\"series_value\",):\n",
        "    col_names = []\n",
        "    col_types = []\n",
        "    all_data = {}\n",
        "    line_count = 0\n",
        "    frequency = None\n",
        "    forecast_horizon = None\n",
        "    contain_missing_values = None\n",
        "    contain_equal_length = None\n",
        "    found_data_tag = False\n",
        "    found_data_section = False\n",
        "    started_reading_data_section = False\n",
        "\n",
        "    with open(full_file_path_and_name, \"r\", encoding=\"cp1252\") as file:\n",
        "        for line in file:\n",
        "            # Strip white space from start/end of line\n",
        "            line = line.strip()\n",
        "\n",
        "            if line:\n",
        "                if line.startswith(\"@\"):  # Read meta-data\n",
        "                    if not line.startswith(\"@data\"):\n",
        "                        line_content = line.split(\" \")\n",
        "                        if line.startswith(\"@attribute\"):\n",
        "                            if (\n",
        "                                len(line_content) != 3\n",
        "                            ):  # Attributes have both name and type\n",
        "                                raise Exception(\"Invalid meta-data specification.\")\n",
        "\n",
        "                            col_names.append(line_content[1])\n",
        "                            col_types.append(line_content[2])\n",
        "                        else:\n",
        "                            if (\n",
        "                                len(line_content) != 2\n",
        "                            ):  # Other meta-data have only values\n",
        "                                raise Exception(\"Invalid meta-data specification.\")\n",
        "\n",
        "                            if line.startswith(\"@frequency\"):\n",
        "                                frequency = line_content[1]\n",
        "                            elif line.startswith(\"@horizon\"):\n",
        "                                forecast_horizon = int(line_content[1])\n",
        "                            elif line.startswith(\"@missing\"):\n",
        "                                contain_missing_values = bool(\n",
        "                                    strtobool(line_content[1])\n",
        "                                )\n",
        "                            elif line.startswith(\"@equallength\"):\n",
        "                                contain_equal_length = bool(strtobool(line_content[1]))\n",
        "\n",
        "                    else:\n",
        "                        if len(col_names) == 0:\n",
        "                            raise Exception(\n",
        "                                \"Missing attribute section. Attribute section must come before data.\"\n",
        "                            )\n",
        "\n",
        "                        found_data_tag = True\n",
        "                elif not line.startswith(\"#\"):\n",
        "                    if len(col_names) == 0:\n",
        "                        raise Exception(\n",
        "                            \"Missing attribute section. Attribute section must come before data.\"\n",
        "                        )\n",
        "                    elif not found_data_tag:\n",
        "                        raise Exception(\"Missing @data tag.\")\n",
        "                    else:\n",
        "                        if not started_reading_data_section:\n",
        "                            started_reading_data_section = True\n",
        "                            found_data_section = True\n",
        "                            all_series = []\n",
        "\n",
        "                            for col in col_names:\n",
        "                                all_data[col] = []\n",
        "\n",
        "                        full_info = line.split(\":\")\n",
        "\n",
        "                        if len(full_info) != (len(col_names) + 1):\n",
        "                            raise Exception(\"Missing attributes/values in series.\")\n",
        "\n",
        "                        series = full_info[len(full_info) - 1]\n",
        "                        series = series.split(\",\")\n",
        "\n",
        "                        if len(series) == 0:\n",
        "                            raise Exception(\n",
        "                                \"A given series should contains a set of comma separated numeric values. At least one numeric value should be there in a series. Missing values should be indicated with ? symbol\"\n",
        "                            )\n",
        "\n",
        "                        numeric_series = []\n",
        "\n",
        "                        for val in series:\n",
        "                            if val == \"?\":\n",
        "                                numeric_series.append(replace_missing_vals_with)\n",
        "                            else:\n",
        "                                numeric_series.append(float(val))\n",
        "\n",
        "                        if numeric_series.count(replace_missing_vals_with) == len(\n",
        "                            numeric_series\n",
        "                        ):\n",
        "                            raise Exception(\n",
        "                                \"All series values are missing. A given series should contains a set of comma separated numeric values. At least one numeric value should be there in a series.\"\n",
        "                            )\n",
        "\n",
        "                        all_series.append(pd.Series(numeric_series).array)\n",
        "\n",
        "                        for i in range(len(col_names)):\n",
        "                            att_val = None\n",
        "                            if col_types[i] == \"numeric\":\n",
        "                                att_val = int(full_info[i])\n",
        "                            elif col_types[i] == \"string\":\n",
        "                                att_val = str(full_info[i])\n",
        "                            elif col_types[i] == \"date\":\n",
        "                                att_val = datetime.strptime(\n",
        "                                    full_info[i], \"%Y-%m-%d %H-%M-%S\"\n",
        "                                )\n",
        "                            else:\n",
        "                                raise Exception(\n",
        "                                    \"Invalid attribute type.\"\n",
        "                                )  # Currently, the code supports only numeric, string and date types. Extend this as required.\n",
        "\n",
        "                            if att_val is None:\n",
        "                                raise Exception(\"Invalid attribute value.\")\n",
        "                            else:\n",
        "                                all_data[col_names[i]].append(att_val)\n",
        "\n",
        "                line_count = line_count + 1\n",
        "\n",
        "        if line_count == 0:\n",
        "            raise Exception(\"Empty file.\")\n",
        "        if len(col_names) == 0:\n",
        "            raise Exception(\"Missing attribute section.\")\n",
        "        if not found_data_section:\n",
        "            raise Exception(\"Missing series information under data section.\")\n",
        "\n",
        "        all_data[value_column_name] = all_series\n",
        "        loaded_data = pd.DataFrame(all_data)\n",
        "\n",
        "        return (\n",
        "            loaded_data,\n",
        "            frequency,\n",
        "            forecast_horizon,\n",
        "            contain_missing_values,\n",
        "            contain_equal_length,\n",
        "        )\n",
        "# Example of usage\n",
        "# loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(\"TSForecasting/tsf_data/sample.tsf\")\n"
      ],
      "metadata": {
        "id": "BaA4_h2USgB8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trim_dataframe(filename,columnname):\n",
        "    loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(filename)\n",
        "    building_data = loaded_data[loaded_data['series_name'].str.contains(columnname)]\n",
        "    max_start_timestamp = building_data['start_timestamp'].max()\n",
        "    building_data['num_timestamps'] = building_data['series_value'].apply(len)\n",
        "    min_timestamps = building_data['num_timestamps'].min()\n",
        "    building_data['uniform_series'] = building_data['series_value'].apply(lambda x: x[-min_timestamps:])\n",
        "    building_data['num_timestamps'] = building_data['uniform_series'].apply(len)\n",
        "    return building_data['uniform_series']\n"
      ],
      "metadata": {
        "id": "XNtA88ETUVQ0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def impute_dataframe(dataframe, rank, time_lags, burn_iter, gibbs_iter, option = \"factor\"):\n",
        "    dense_tensor = dataframe\n",
        "    dim = dense_tensor.shape\n",
        "    list_of_arrays = [np.array(series) for series in dense_tensor]\n",
        "    # Stack these arrays vertically to form a 2D matrix\n",
        "    dense_mat_2d = np.vstack(list_of_arrays)\n",
        "    sparse_mat = dense_mat_2d.copy()\n",
        "    dense_mat_2d = np.where(dense_mat_2d == 'NaN', np.nan, dense_mat_2d).astype(float)\n",
        "    sparse_mat = np.where(sparse_mat == 'NaN', np.nan, sparse_mat).astype(float)\n",
        "    del dense_tensor\n",
        "    dim1, dim2 = sparse_mat.shape\n",
        "    init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
        "    mat, W, X, A= BTMF_original.BTMF(dense_mat_2d, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter)\n",
        "    # Assuming you have column names and indices stored\n",
        "    df=pd.DataFrame(mat)\n",
        "    return df"
      ],
      "metadata": {
        "id": "y-YamZO0UXMn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NaN_df=trim_dataframe(\"/content/drive/MyDrive/Exeter-Placement/Challenge/phase_1 data/phase_1_data/phase_1_data.tsf\",\"Building\")\n",
        "time_lags = np.array([1, 4, 96])\n",
        "burn_iter=0\n",
        "gibbs_iter=1\n",
        "rank=10\n",
        "\n",
        "df=impute_dataframe(NaN_df,rank,time_lags,burn_iter,gibbs_iter)\n",
        "scaler = MinMaxScaler()\n",
        "df_scaled = pd.DataFrame(scaler.fit_transform(df.T).T, columns=df.columns, index=df.index)\n",
        "\n",
        "sequence_length = 8640  # 3 months of data\n",
        "prediction_length = 2880  # 1 month of data\n",
        "\n",
        "train_end = 41572 - prediction_length - sequence_length\n",
        "train_data = df_scaled.iloc[:, :train_end]\n",
        "validation_data = df_scaled.iloc[:, train_end:train_end+sequence_length]\n",
        "test_data = df_scaled.iloc[:, train_end+sequence_length:train_end+2*sequence_length]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ubX1fu3UaW1",
        "outputId": "fa7f80bb-bc6d-4b06-db5f-a42f831c40c1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-2cfb269e37d8>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  building_data['num_timestamps'] = building_data['series_value'].apply(len)\n",
            "<ipython-input-3-2cfb269e37d8>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  building_data['uniform_series'] = building_data['series_value'].apply(lambda x: x[-min_timestamps:])\n",
            "<ipython-input-3-2cfb269e37d8>:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  building_data['num_timestamps'] = building_data['uniform_series'].apply(len)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BTMF Iteration: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(data, seq_length, pred_length):\n",
        "    sequences = []\n",
        "    target_sequences = []\n",
        "\n",
        "    for i in range(len(data.columns) - seq_length - pred_length + 1):\n",
        "        sequences.append(data.iloc[:, i:i+seq_length].values)\n",
        "        target_sequences.append(data.iloc[:, i+seq_length:i+seq_length+pred_length].values)\n",
        "\n",
        "    return np.array(sequences), np.array(target_sequences)"
      ],
      "metadata": {
        "id": "y52n-XhaUc06"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, Y_train = create_sequences(train_data, sequence_length, prediction_length)\n",
        "X_val, Y_val = create_sequences(validation_data, sequence_length, prediction_length)\n"
      ],
      "metadata": {
        "id": "MNtl0uZ4Ud6Q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def positional_encoding(seq_length, d_model):\n",
        "    position = np.arange(seq_length)[:, np.newaxis]\n",
        "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
        "    pos_enc = np.zeros((d_model, seq_length))\n",
        "    pos_enc[0::2, :] = np.sin(position * div_term).T\n",
        "    pos_enc[1::2, :] = np.cos(position * div_term).T\n",
        "    return pos_enc[np.newaxis, :, :]\n"
      ],
      "metadata": {
        "id": "VVv08J8AUgET"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 6  # Number of sensors\n",
        "# Generate positional encoding\n",
        "pos_enc = positional_encoding(sequence_length, d_model)\n",
        "print(pos_enc.shape, type(pos_enc))\n",
        "# Add positional encoding to X_train\n",
        "X_val = validation_data.values[np.newaxis, :, :]\n",
        "X_train += pos_enc\n",
        "X_val += pos_enc\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzduyfWeUgkh",
        "outputId": "b17c9cd9-d341-4117-bad6-849c91c55e31"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 6, 8640) <class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        assert d_model % self.num_heads == 0\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = nn.Linear(8640, d_model)\n",
        "        self.wk = nn.Linear(8640, d_model)\n",
        "        self.wv = nn.Linear(8640, d_model)\n",
        "\n",
        "        self.dense = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, v, k, q, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        q = self.split_heads(self.wq(q), batch_size)\n",
        "        k = self.split_heads(self.wk(k), batch_size)\n",
        "        v = self.split_heads(self.wv(v), batch_size)\n",
        "\n",
        "        matmul_qk = torch.matmul(q, k.permute(0, 1, 3, 2))\n",
        "        d_k = self.depth ** 0.5\n",
        "        scaled_attention_logits = matmul_qk / d_k\n",
        "\n",
        "        if mask is not None:\n",
        "            scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "        attention_weights = F.softmax(scaled_attention_logits, dim=-1)\n",
        "        output = torch.matmul(attention_weights, v)\n",
        "\n",
        "        output = output.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
        "        return self.dense(output), attention_weights"
      ],
      "metadata": {
        "id": "3AMz8PHvU2sA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = nn.Sequential(\n",
        "          nn.Linear(d_model, dff),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(dff, d_model)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.layernorm1 = nn.LayerNorm(d_model)\n",
        "        self.layernorm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, training, mask=None):\n",
        "        print(\"start\")\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2\n"
      ],
      "metadata": {
        "id": "d19EaOAcVJdI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, max_position_encoding, dropout_rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "        print(\"dnfhgafg\")\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.pos_encoding = torch.tensor(positional_encoding(max_position_encoding, d_model), dtype=torch.float32)\n",
        "        self.enc_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, dff, dropout_rate) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        print(type(self.pos_encoding))  # Check the type of self.pos_encoding\n",
        "        print(self.pos_encoding.device)  # Check the device of self.pos_encoding if it's a tensor\n",
        "        print(\"yo\")\n",
        "    def forward(self, x, training, mask=None):\n",
        "        print(\"heloooooo\")\n",
        "        seq_len = x.size(1)\n",
        "        self.pos_encoding = self.pos_encoding.clone().detach().to(x.device)\n",
        "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
        "        #x += self.pos_encoding[:, :seq_len].to(x.device)\n",
        "        #x += self.pos_encoding[:, :seq_len, :x.size(2)]\n",
        "        print(\"x shape\",x.shape)\n",
        "        print(\"pos encoding shape\",self.pos_encoding[:, :seq_len, :].shape)\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "        print(x.shape)\n",
        "        print(self.pos_encoding[:, :seq_len, :x.size(2)].shape)\n",
        "        x = self.dropout(x)\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "        return x\n",
        "\"\"\"\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, max_position_encoding, dropout_rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Create a positional encoding tensor with a sequence length of 8640\n",
        "        self.pos_encoding = torch.tensor(positional_encoding(6, d_model), dtype=torch.float32)\n",
        "        #self.pos_encoding = torch.tensor(positional_encoding(8640, d_model), dtype=torch.float32)\n",
        "\n",
        "        self.enc_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, dff, dropout_rate) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, training, mask=None):\n",
        "        x = x.view(batch_size, -1)\n",
        "        seq_len = x.size(1)\n",
        "        self.pos_encoding = self.pos_encoding.clone().detach().to(x.device)\n",
        "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
        "\n",
        "        # Slice the positional encoding tensor to match the sequence length of x\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "_y5fZmsoVMy6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, dff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dff, d_model)\n",
        "        )\n",
        "\n",
        "        self.layernorm1 = nn.LayerNorm(d_model)\n",
        "        self.layernorm2 = nn.LayerNorm(d_model)\n",
        "        self.layernorm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "        self.dropout3 = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, enc_output, training, look_ahead_mask=None, padding_mask=None):\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2\n"
      ],
      "metadata": {
        "id": "A3nin5F6V7_0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, max_position_encoding, dropout_rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.pos_encoding = torch.tensor(positional_encoding(max_position_encoding, d_model), dtype=torch.float32)\n",
        "\n",
        "\n",
        "        self.dec_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, dff, dropout_rate) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, enc_output, training, look_ahead_mask=None, padding_mask=None):\n",
        "        seq_len = x.size(1)\n",
        "        attention_weights = {}\n",
        "\n",
        "        # Remove the embedding line\n",
        "        # x = self.embedding(x) * torch.sqrt(torch.tensor(self.d_model))\n",
        "\n",
        "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
        "        #x += self.pos_encoding[:, :seq_len].to(x.device)\n",
        "        x += self.pos_encoding[:, :seq_len, :x.size(2)]\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
        "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "        return x, attention_weights\n",
        "\n"
      ],
      "metadata": {
        "id": "oM2quFbkV9vw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, dropout_rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, dropout_rate)\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, dropout_rate)\n",
        "\n",
        "        self.final_layer = nn.Linear(d_model, target_vocab_size)\n",
        "\n",
        "    def forward(self, inp, tar, training, enc_padding_mask=None, look_ahead_mask=None, dec_padding_mask=None):\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
        "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)\n",
        "        return final_output, attention_weights\n"
      ],
      "metadata": {
        "id": "UAr55WK-V_kv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "input_vocab_size = 512\n",
        "target_vocab_size = 512\n",
        "max_position_encoding_input = 9000  # A bit more than 8640 to ensure coverage\n",
        "max_position_encoding_target = 9000\n",
        "\n",
        "# Create the Transformer model instance\n",
        "model = Transformer(num_layers=6, d_model=512, num_heads=8, dff=2048,\n",
        "                    input_vocab_size=input_vocab_size, target_vocab_size=target_vocab_size,\n",
        "                    pe_input=max_position_encoding_input, pe_target=max_position_encoding_target)\n",
        "\n",
        "# Now, you can define your optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "CHNB_7c3Wqwf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with a learning rate of 0.001\n"
      ],
      "metadata": {
        "id": "Y-7Az3tkWIK1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.Y[idx]\n",
        "\n",
        "train_dataset = TimeSeriesDataset(X_train, Y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "_3ofdPV0b1fW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(num_layers=6, d_model=512, num_heads=8, dff=2048,\n",
        "                    input_vocab_size=512, target_vocab_size=512,\n",
        "                    pe_input=9000, pe_target=9000)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "wbGk5J7Gb-Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        data = data.float()\n",
        "        print(\"Input data shape:\", data.shape)\n",
        "        target = target.float()\n",
        "\n",
        "        # Forward pass\n",
        "        # Exclude the last time step from target as input to the decoder\n",
        "        outputs, _ = model(data, target[:, :-1], training=True)\n",
        "\n",
        "        # Compute the loss\n",
        "        # Exclude the first time step from target to match the shape of outputs\n",
        "        loss = criterion(outputs, target[:, 1:])\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "7_HkKUhuWIz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S3k8vaospFic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Qd5pXQdZpvkv"
      }
    }
  ]
}